{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages\n",
    "\n",
    "We use pydicom, opencv and lmdb for reading and interpreting DICOM files from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all packages\n"
     ]
    }
   ],
   "source": [
    "import cv2, dicom, lmdb, json\n",
    "import os, random, re, sys, shutil\n",
    "import fnmatch, subprocess\n",
    "from IPython.utils import io\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.ndimage.morphology import binary_erosion\n",
    "from scipy.fftpack import fftn, ifftn\n",
    "from scipy.signal import argrelmin, correlate\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
    "\n",
    "CAFFE_ROOT = \"/home/ubuntu/caffe\"\n",
    "caffe_path = os.path.join(CAFFE_ROOT, \"python\")\n",
    "if caffe_path not in sys.path:\n",
    "    sys.path.insert(0, caffe_path)\n",
    "\n",
    "import caffe\n",
    "\n",
    "print \"Successfully imported all packages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined all parameters\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# PARAMETERS\n",
    "#\n",
    "\n",
    "\n",
    "# number of bins to use in histogram for gaussian regression\n",
    "NUM_BINS = 100\n",
    "# number of standard deviations past which we will consider a pixel an outlier\n",
    "STD_MULTIPLIER = 2\n",
    "# number of points of our interpolated dataset to consider when searching for\n",
    "# a threshold value; the function by default is interpolated over 1000 points,\n",
    "# so 250 will look at the half of the points that is centered around the known\n",
    "# myocardium pixel\n",
    "THRESHOLD_AREA = 250\n",
    "# number of pixels on the line within which to search for a connected component\n",
    "# in a thresholded image, increase this to look for components further away\n",
    "COMPONENT_INDEX_TOLERANCE = 20\n",
    "# number of angles to search when looking for the correct orientation\n",
    "ANGLE_SLICES = 36\n",
    "\n",
    "print \"Defined all parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "\n",
    "This class traverses the input data and reads the input data set (DICOM) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    dataset_count = 0\n",
    "\n",
    "    def __init__(self, directory, subdir):\n",
    "        # deal with any intervening directories\n",
    "        while True:\n",
    "            subdirs = next(os.walk(directory))[1]\n",
    "            if len(subdirs) == 1:\n",
    "                directory = os.path.join(directory, subdirs[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        slices = []\n",
    "        for s in subdirs:\n",
    "            m = re.match(\"sax_(\\d+)\", s)\n",
    "            if m is not None:\n",
    "                slices.append(int(m.group(1)))\n",
    "\n",
    "        slices_map = {}\n",
    "        first = True\n",
    "        times = []\n",
    "        for s in slices:\n",
    "            files = next(os.walk(os.path.join(directory, \"sax_%d\" % s)))[2]\n",
    "            offset = None\n",
    "\n",
    "            for f in files:\n",
    "                m = re.match(\"IM-(\\d{4,})-(\\d{4})\\.dcm\", f)\n",
    "                if m is not None:\n",
    "                    if first:\n",
    "                        times.append(int(m.group(2)))\n",
    "                    if offset is None:\n",
    "                        offset = int(m.group(1))\n",
    "\n",
    "            first = False\n",
    "            slices_map[s] = offset\n",
    "\n",
    "        self.directory = directory\n",
    "        self.time = sorted(times)\n",
    "        self.slices = sorted(slices)\n",
    "        self.slices_map = slices_map\n",
    "        Dataset.dataset_count += 1\n",
    "        self.name = subdir\n",
    "\n",
    "    def _filename(self, s, t):\n",
    "        return os.path.join(self.directory,\"sax_%d\" % s, \"IM-%04d-%04d.dcm\" % (self.slices_map[s], t))\n",
    "\n",
    "    def _read_dicom_image(self, filename):\n",
    "        d = dicom.read_file(filename)\n",
    "        img = d.pixel_array\n",
    "        return np.array(img)\n",
    "\n",
    "    def _read_all_dicom_images(self):\n",
    "        f1 = self._filename(self.slices[0], self.time[0])\n",
    "        d1 = dicom.read_file(f1)\n",
    "        (x, y) = d1.PixelSpacing\n",
    "        (x, y) = (float(x), float(y))\n",
    "        f2 = self._filename(self.slices[1], self.time[0])\n",
    "        d2 = dicom.read_file(f2)\n",
    "\n",
    "        # try a couple of things to measure distance between slices\n",
    "        try:\n",
    "            dist = np.abs(d2.SliceLocation - d1.SliceLocation)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                dist = d1.SliceThickness\n",
    "            except AttributeError:\n",
    "                dist = 8  # better than nothing...\n",
    "\n",
    "        self.images = np.array([[self._read_dicom_image(self._filename(d, i))\n",
    "                                 for i in self.time]\n",
    "                                for d in self.slices])\n",
    "        self.dist = dist\n",
    "        self.area_multiplier = x * y\n",
    "\n",
    "    def load(self):\n",
    "        self._read_all_dicom_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumes dataset is loaded, call dataset.load()\n",
    "def segment_dataset(dataset):\n",
    "    images = dataset.images\n",
    "    dist = dataset.dist\n",
    "    areaMultiplier = dataset.area_multiplier\n",
    "    # shape: num slices, num snapshots, rows, columns\n",
    "    log(\"Calculating rois...\", 1)\n",
    "    rois, circles = calc_rois(images)\n",
    "    log(\"Calculating areas...\", 1)\n",
    "    all_masks, all_areas = calc_all_areas(images, rois, circles)\n",
    "    log(\"Calculating volumes...\", 1)\n",
    "    area_totals = [calc_total_volume(a, areaMultiplier, dist)\n",
    "                   for a in all_areas]\n",
    "    log(\"Calculating ef...\", 1)\n",
    "    edv = max(area_totals)\n",
    "    esv = min(area_totals)\n",
    "    ef = (edv - esv) / edv\n",
    "    log(\"Done, ef is %f\" % ef, 1)\n",
    "\n",
    "    save_masks_to_dir(dataset, all_masks)\n",
    "\n",
    "    output = {}\n",
    "    output[\"edv\"] = edv\n",
    "    output[\"esv\"] = esv\n",
    "    output[\"ef\"] = ef\n",
    "    output[\"areas\"] = all_areas.tolist()\n",
    "    f = open(\"output/%s/output.json\" % dataset.name, \"w\")\n",
    "    json.dump(output, f, indent=2)\n",
    "    f.close()\n",
    "    dataset.edv = edv\n",
    "    dataset.esv = esv\n",
    "    dataset.ef = ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_masks_to_dir(dataset, all_masks):\n",
    "    imgs, labels = [], []\n",
    "    os.mkdir(\"output/%s\" % dataset.name)\n",
    "    if os.path.exists(\"input\"):\n",
    "        shutil.rmtree(\"input\")\n",
    "    os.mkdir(\"input\")\n",
    "    os.mkdir(\"input/%s\" % dataset.name)\n",
    "    for s in range(len(dataset.slices)):\n",
    "        os.mkdir(\"output/%s/slice%02d\" % (dataset.name, s))\n",
    "        os.mkdir(\"input/%s/slice%02d\" % (dataset.name, s))\n",
    "        for t in range(len(dataset.time)):\n",
    "            mask = all_masks[t][s]\n",
    "            image.imsave(\"output/%s/slice%02d/time%02d_mask.png\" %\n",
    "                         (dataset.name, s, t), mask)\n",
    "            labels.append(mask.astype('uint8'))\n",
    "            image_save = dataset.images[s][t]\n",
    "            image.imsave(\"input/%s/slice%02d/time%02d_DICOM.png\" %\n",
    "                         (dataset.name, s, t), image_save)\n",
    "            imgs.append(image_save.astype('int64'))\n",
    "            \n",
    "        print \"# of images: {0}, # of labels: {1}\".format(len(imgs), len(labels))\n",
    "\n",
    "    for lmdb_name in [\"lmdb_img_name\", \"lmdb_label_name\"]:\n",
    "        db_path = os.path.abspath(lmdb_name)\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "    counter_img = 0\n",
    "    counter_label = 0\n",
    "    batchsz = 100\n",
    "    print(\"Processing {:d} images and labels...\".format(len(imgs)))\n",
    "    for i in xrange(int(np.ceil(len(imgs) / float(batchsz)))):\n",
    "        batch_imgs = imgs[(batchsz*i):(batchsz*(i+1))]\n",
    "        batch_labels = labels[(batchsz*i):(batchsz*(i+1))]\n",
    "        if len(batch_imgs) == 0:\n",
    "            break\n",
    "        db_imgs = lmdb.open(\"lmdb_img_name\", map_size=1e12)\n",
    "        with db_imgs.begin(write=True) as txn_img:          \n",
    "            for img in batch_imgs:\n",
    "                datum = caffe.io.array_to_datum(np.expand_dims(img, axis=0))\n",
    "                txn_img.put(\"{:0>10d}\".format(counter_img), datum.SerializeToString())\n",
    "                counter_img += 1\n",
    "        print(\"Processed {:d} images\".format(counter_img))\n",
    "        db_labels = lmdb.open(\"lmdb_label_name\", map_size=1e12)\n",
    "        with db_labels.begin(write=True) as txn_label:\n",
    "            for lbl in batch_labels:\n",
    "                datum = caffe.io.array_to_datum(np.expand_dims(lbl, axis=0))\n",
    "                txn_label.put(\"{:0>10d}\".format(counter_label), datum.SerializeToString())\n",
    "                counter_label += 1\n",
    "        print(\"Processed {:d} labels\".format(counter_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_rois(images):\n",
    "    (num_slices, _, _, _) = images.shape\n",
    "    log(\"Calculating mean...\", 2)\n",
    "    dc = np.mean(images, 1)\n",
    "\n",
    "    def get_H1(i):\n",
    "        log(\"Fourier transforming on slice %d...\" % i, 3)\n",
    "        ff = fftn(images[i])\n",
    "        print \"ff\"+str(np.shape(ff))\n",
    "        first_harmonic = ff[1, :, :]\n",
    "        print \"first_harmonic\"+str(np.shape(first_harmonic))\n",
    "        log(\"Inverse Fourier transforming on slice %d...\" % i, 3)\n",
    "        result = np.absolute(ifftn(first_harmonic))\n",
    "        log(\"Performing Gaussian blur on slice %d...\" % i, 3)\n",
    "        result = cv2.GaussianBlur(result, (5, 5), 0)\n",
    "        print \"result\" + str(np.shape(result))\n",
    "        return result\n",
    "\n",
    "    log(\"Performing Fourier transforms...\", 2)\n",
    "    h1s = np.array([get_H1(i) for i in range(num_slices)])\n",
    "    print \"h1s\" + str(np.shape(h1s))\n",
    "    m = np.max(h1s) * 0.05\n",
    "    h1s[h1s < m] = 0\n",
    "\n",
    "    log(\"Applying regression filter...\", 2)\n",
    "    regress_h1s = regression_filter(h1s)\n",
    "    log(\"Post-processing filtered images...\", 2)\n",
    "    proc_regress_h1s, coords = post_process_regression(regress_h1s)\n",
    "    log(\"Determining ROIs...\", 2)\n",
    "    rois, circles = get_ROIs(dc, proc_regress_h1s, coords)\n",
    "    return rois, circles\n",
    "\n",
    "\n",
    "def calc_all_areas(images, rois, circles):\n",
    "    closest_slice = get_closest_slice(rois)\n",
    "    (_, times, _, _) = images.shape\n",
    "\n",
    "    def calc_areas(time):\n",
    "        log(\"Calculating areas at time %d...\" % time, 2)\n",
    "        mask, mean = locate_lv_blood_pool(images, rois, circles, closest_slice,\n",
    "                                          time)\n",
    "        masks, areas = propagate_segments(images, rois, mask, mean,\n",
    "                                          closest_slice, time)\n",
    "        return (masks, areas)\n",
    "\n",
    "    result = np.transpose(map(calc_areas, range(times)))\n",
    "    all_masks = result[0]\n",
    "    all_areas = result[1]\n",
    "    return all_masks, all_areas\n",
    "\n",
    "\n",
    "def calc_total_volume(areas, area_multiplier, dist):\n",
    "    slices = np.array(sorted(areas.keys()))\n",
    "    modified = [areas[i] * area_multiplier for i in slices]\n",
    "    vol = 0\n",
    "    for i in slices[:-1]:\n",
    "        a, b = modified[i], modified[i+1]\n",
    "        subvol = (dist/3.0) * (a + np.sqrt(a*b) + b)\n",
    "        vol += subvol / 1000.0  # conversion to mL\n",
    "    return vol\n",
    "\n",
    "\n",
    "def get_centroid(img):\n",
    "    nz = np.nonzero(img)\n",
    "    pxls = np.transpose(nz)\n",
    "    weights = img[nz]\n",
    "    avg = np.average(pxls, axis=0, weights=weights)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def regress_centroids(cs):\n",
    "    num_slices = len(cs)\n",
    "    y_centroids = cs[:, 0]\n",
    "    x_centroids = cs[:, 1]\n",
    "    z_values = np.array(range(num_slices))\n",
    "\n",
    "    (xslope, xintercept, _, _, _) = linregress(z_values, x_centroids)\n",
    "    (yslope, yintercept, _, _, _) = linregress(z_values, y_centroids)\n",
    "\n",
    "    return (xslope, xintercept, yslope, yintercept)\n",
    "\n",
    "\n",
    "def get_weighted_distances(imgs, coords, xs, xi, ys, yi):\n",
    "    a = np.array([0, yi, xi])\n",
    "    n = np.array([1, ys, xs])\n",
    "\n",
    "    zeros = np.zeros(3)\n",
    "\n",
    "    def dist(p):\n",
    "        to_line = (a - p) - (np.dot((a - p), n) * n)\n",
    "        d = euclidean(zeros, to_line)\n",
    "        return d\n",
    "\n",
    "    def weight(p):\n",
    "        (z, y, x) = p\n",
    "        return imgs[z, y, x]\n",
    "\n",
    "    dists = np.array([dist(c) for c in coords])\n",
    "    weights = np.array([weight(c) for c in coords])\n",
    "    return (coords, dists, weights)\n",
    "\n",
    "\n",
    "def gaussian_fit(dists, weights):\n",
    "    # based on http://stackoverflow.com/questions/11507028/fit-a-gaussian-function\n",
    "    (x, y) = histogram_transform(dists, weights)\n",
    "    fivep = int(len(x) * 0.05)\n",
    "    xtmp = x\n",
    "    ytmp = y\n",
    "    fromFront = False\n",
    "    while True:\n",
    "        if len(xtmp) == 0 and len(ytmp) == 0:\n",
    "            if fromFront:\n",
    "                # well we failed\n",
    "                idx = np.argmax(y)\n",
    "                xmax = x[idx]\n",
    "                p0 = [max(y), xmax, xmax]\n",
    "                (A, mu, sigma) = p0\n",
    "                return mu, sigma, lambda x: gauss(x, A, mu, sigma)\n",
    "            else:\n",
    "                fromFront = True\n",
    "                xtmp = x\n",
    "                ytmp = y\n",
    "\n",
    "        idx = np.argmax(ytmp)\n",
    "        xmax = xtmp[idx]\n",
    "\n",
    "        def gauss(x, *p):\n",
    "            A, mu, sigma = p\n",
    "            return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "        p0 = [max(ytmp), xmax, xmax]\n",
    "        try:\n",
    "            coeff, var_matrix = curve_fit(gauss, xtmp, ytmp, p0=p0)\n",
    "            (A, mu, sigma) = coeff\n",
    "            return (mu, sigma, lambda x: gauss(x, A, mu, sigma))\n",
    "        except RuntimeError:\n",
    "            if fromFront:\n",
    "                xtmp = xtmp[fivep:]\n",
    "                ytmp = ytmp[fivep:]\n",
    "            else:\n",
    "                xtmp = xtmp[:-fivep]\n",
    "                ytmp = ytmp[:-fivep]\n",
    "\n",
    "\n",
    "def histogram_transform(values, weights):\n",
    "    hist, bins = np.histogram(values, bins=NUM_BINS, weights=weights)\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    bin_centers = bins[:-1] + (bin_width / 2)\n",
    "\n",
    "    return (bin_centers, hist)\n",
    "\n",
    "\n",
    "def get_outliers(coords, dists, weights):\n",
    "    fivep = int(len(weights) * 0.05)\n",
    "    ctr = 1\n",
    "    while True:\n",
    "        (mean, std, fn) = gaussian_fit(dists, weights)\n",
    "        low_values = dists < (mean - STD_MULTIPLIER*np.abs(std))\n",
    "        high_values = dists > (mean + STD_MULTIPLIER*np.abs(std))\n",
    "        outliers = np.logical_or(low_values, high_values)\n",
    "        if len(coords[outliers]) == len(coords):\n",
    "            weights[-fivep*ctr:] = 0\n",
    "            ctr += 1\n",
    "        else:\n",
    "            return coords[outliers]\n",
    "\n",
    "\n",
    "def regress_and_filter_distant(imgs):\n",
    "    centroids = np.array([get_centroid(img) for img in imgs])\n",
    "    raw_coords = np.transpose(np.nonzero(imgs))\n",
    "    (xslope, xintercept, yslope, yintercept) = regress_centroids(centroids)\n",
    "    (coords, dists, weights) = get_weighted_distances(imgs, raw_coords, xslope,\n",
    "                                                      xintercept, yslope,\n",
    "                                                      yintercept)\n",
    "    outliers = get_outliers(coords, dists, weights)\n",
    "    imgs_cpy = np.copy(imgs)\n",
    "    for c in outliers:\n",
    "        (z, x, y) = c\n",
    "        imgs_cpy[z, x, y] = 0\n",
    "    return imgs_cpy\n",
    "\n",
    "\n",
    "def regression_filter(imgs):\n",
    "    condition = True\n",
    "    iternum = 0\n",
    "    while(condition):\n",
    "        log(\"Beginning iteration %d of regression...\" % iternum, 3)\n",
    "        iternum += 1\n",
    "        imgs_filtered = regress_and_filter_distant(imgs)\n",
    "        c1 = get_centroid(imgs)\n",
    "        print \"c1\" + str(c1)\n",
    "        c2 = get_centroid(imgs_filtered)\n",
    "        dc = np.linalg.norm(c1 - c2)\n",
    "        imgs = imgs_filtered\n",
    "        condition = (dc > 1.0)  # because python has no do-while loops\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def post_process_regression(imgs):\n",
    "    (numimgs, _, _) = imgs.shape\n",
    "    centroids = np.array([get_centroid(img) for img in imgs])\n",
    "    log(\"Performing final centroid regression...\", 3)\n",
    "    (xslope, xintercept, yslope, yintercept) = regress_centroids(centroids)\n",
    "    imgs_cpy = np.copy(imgs)\n",
    "\n",
    "    def filter_one_img(zlvl):\n",
    "        points_on_zlvl = np.transpose(imgs[zlvl].nonzero())\n",
    "        points_on_zlvl = np.insert(points_on_zlvl, 0, zlvl, axis=1)\n",
    "        (coords, dists, weights) = get_weighted_distances(imgs, points_on_zlvl,\n",
    "                                                          xslope, xintercept,\n",
    "                                                          yslope, yintercept)\n",
    "        outliers = get_outliers(coords, dists, weights)\n",
    "        for c in outliers:\n",
    "            (z, x, y) = c\n",
    "            imgs_cpy[z, x, y] = 0\n",
    "\n",
    "    log(\"Final image filtering...\", 3)\n",
    "    for z in range(numimgs):\n",
    "        log(\"Filtering image %d of %d...\" % (z+1, numimgs), 4)\n",
    "        filter_one_img(z)\n",
    "\n",
    "    return (imgs_cpy, (xslope, xintercept, yslope, yintercept))\n",
    "\n",
    "\n",
    "def floats_draw_circle(img, center, r, color, thickness):\n",
    "    (x, y) = center\n",
    "    x, y = int(np.round(x)), int(np.round(y))\n",
    "    r = int(np.round(r))\n",
    "    cv2.circle(img, center=(x, y), radius=r, color=color, thickness=thickness)\n",
    "\n",
    "\n",
    "def filled_ratio_of_circle(img, center, r):\n",
    "    mask = np.zeros_like(img)\n",
    "    floats_draw_circle(mask, center, r, 1, -1)\n",
    "    masked = mask * img\n",
    "    (x, _) = np.nonzero(mask)\n",
    "    (x2, _) = np.nonzero(masked)\n",
    "    if x.size == 0:\n",
    "        return 0\n",
    "    return float(x2.size) / x.size\n",
    "\n",
    "\n",
    "def circle_smart_radius(img, center):\n",
    "    domain = np.arange(1, 100)\n",
    "    (xintercept, yintercept) = center\n",
    "\n",
    "    def ratio(r):\n",
    "        return filled_ratio_of_circle(img, (xintercept, yintercept), r)*r\n",
    "\n",
    "    y = np.array([ratio(d) for d in domain])\n",
    "    most = np.argmax(y)\n",
    "    return domain[most]\n",
    "\n",
    "\n",
    "def get_ROIs(originals, h1s, regression_params):\n",
    "    (xslope, xintercept, yslope, yintercept) = regression_params\n",
    "    (num_slices, _, _) = h1s.shape\n",
    "    results = []\n",
    "    circles = []\n",
    "    for i in range(num_slices):\n",
    "        log(\"Getting ROI in slice %d...\" % i, 3)\n",
    "        o = originals[i]\n",
    "        h = h1s[i]\n",
    "        ctr = (xintercept + xslope * i, yintercept + yslope * i)\n",
    "        r = circle_smart_radius(h, ctr)\n",
    "        tmp = np.zeros_like(o)\n",
    "        floats_draw_circle(tmp, ctr, r, 1, -1)\n",
    "        results.append(tmp * o)\n",
    "        circles.append((ctr, r))\n",
    "\n",
    "    return (np.array(results), np.array(circles))\n",
    "\n",
    "\n",
    "def bresenham(x0, x1, y0, y1, fn):\n",
    "    # using some pseudocode from\n",
    "    # https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm\n",
    "    # and also https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm\n",
    "    steep = abs(y1-y0) > abs(x1-x0)\n",
    "    if steep:\n",
    "        x0, x1, y0, y1 = y0, y1, x0, x1\n",
    "    if x0 > x1:\n",
    "        x0, x1, y0, y1 = x1, x0, y1, y0\n",
    "\n",
    "    def plot(x, y):\n",
    "        if steep:\n",
    "            fn(y, x)\n",
    "        else:\n",
    "            fn(x, y)\n",
    "\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "\n",
    "    D = 2*np.abs(dy) - dx\n",
    "    plot(x0, y0)\n",
    "    y = y0\n",
    "\n",
    "    for x in range(x0+1, x1+1):  # x0+1 to x1\n",
    "        D = D + 2*np.abs(dy)\n",
    "        if D > 0:\n",
    "            y += np.sign(dy)\n",
    "            D -= 2*dx\n",
    "        plot(x, y)\n",
    "\n",
    "\n",
    "def line_thru(bounds, center, theta):\n",
    "    (xmin, xmax, ymin, ymax) = bounds\n",
    "    (cx, cy) = center\n",
    "\n",
    "    if np.cos(theta) == 0:\n",
    "        return (cx, ymin, cx, ymax)\n",
    "    slope = np.tan(theta)\n",
    "\n",
    "    x0 = xmin\n",
    "    y0 = cy - (cx - xmin) * slope\n",
    "    if y0 < ymin:\n",
    "        y0 = ymin\n",
    "        x0 = max(xmin, cx - ((cy - ymin) / slope))\n",
    "    elif y0 > ymax:\n",
    "        y0 = ymax\n",
    "        x0 = max(xmin, cx - ((cy - ymax) / slope))\n",
    "\n",
    "    x1 = xmax\n",
    "    y1 = cy + (xmax - cx) * slope\n",
    "    if y1 < ymin:\n",
    "        y1 = ymin\n",
    "        x1 = min(xmax, cx + ((ymin - cy) / slope))\n",
    "    elif y1 > ymax:\n",
    "        y1 = ymax\n",
    "        x1 = min(xmax, cx + ((ymax - cy) / slope))\n",
    "\n",
    "    return (x0, x1, y0, y1)\n",
    "\n",
    "\n",
    "def get_line_coords(w, h, cx, cy, theta):\n",
    "    coords = np.floor(np.array(line_thru((0, w-1, 0, h-1), (cx, cy), theta)))\n",
    "    return coords.astype(np.int)\n",
    "\n",
    "\n",
    "def trim_zeros_indices(has_zeros):\n",
    "    first = 0\n",
    "    for i in has_zeros:\n",
    "        if i == 0:\n",
    "            first += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    last = len(has_zeros)\n",
    "    for i in has_zeros[::-1]:\n",
    "        if i == 0:\n",
    "            last -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return first, last\n",
    "\n",
    "\n",
    "def get_line(roi, cx, cy, theta):\n",
    "    (h, w) = roi.shape\n",
    "    (x0, x1, y0, y1) = get_line_coords(w, h, cx, cy, theta)\n",
    "\n",
    "    intensities = []\n",
    "    coords = []\n",
    "\n",
    "    def collect(x, y):\n",
    "        if y < 0 or y >= h or x < 0 or x >= w:\n",
    "            return\n",
    "        intensities.append(roi[y, x])\n",
    "        coords.append((x, y))\n",
    "\n",
    "    bresenham(x0, x1, y0, y1, collect)\n",
    "\n",
    "    def geti(idx):\n",
    "        return intensities[idx]\n",
    "\n",
    "    getiv = np.vectorize(geti)\n",
    "    x = np.arange(0, len(intensities))\n",
    "    y = getiv(x)\n",
    "    first, last = trim_zeros_indices(y)\n",
    "    trimy = y[first:last]\n",
    "    trimcoords = coords[first:last]\n",
    "\n",
    "    trimx = np.arange(0, trimy.size)\n",
    "    return (trimx, trimy, trimcoords)\n",
    "\n",
    "\n",
    "def find_best_angle(roi, circ):\n",
    "    ((cx, cy), r) = circ\n",
    "    results = np.zeros(ANGLE_SLICES)\n",
    "    fns = [None for i in range(ANGLE_SLICES)]\n",
    "\n",
    "    COS_MATCHED_FILTER_FREQ = 2.5\n",
    "\n",
    "    def score_matched(trimx, trimy):\n",
    "        # first, normalize this data\n",
    "        newtrimx = np.linspace(0.0, 1.0, np.size(trimx))\n",
    "        minimum = np.min(trimy)\n",
    "        maximum = np.max(trimy) - minimum\n",
    "        newtrimy = (trimy - minimum) / maximum\n",
    "\n",
    "        filt = 1 - ((np.cos(COS_MATCHED_FILTER_FREQ*2*np.pi*newtrimx)) /\n",
    "                    2 + (0.5))\n",
    "        cr = correlate(newtrimy, filt, mode=\"same\")\n",
    "        return np.max(cr)\n",
    "\n",
    "    for i in range(ANGLE_SLICES):\n",
    "        trimx, trimy, trimcoords = get_line(roi, cx, cy, np.pi*i/ANGLE_SLICES)\n",
    "        score2 = score_matched(trimx, trimy)\n",
    "        results[i] = score2\n",
    "        fns[i] = (UnivariateSpline(trimx, trimy), trimx, trimcoords)\n",
    "\n",
    "    best = np.argmax(results)\n",
    "    return (best * np.pi / ANGLE_SLICES, fns[best])\n",
    "\n",
    "\n",
    "def find_threshold_point(best, best_fn):\n",
    "    fn, trimx, trim_coords = best_fn\n",
    "    dom = np.linspace(np.min(trimx), np.max(trimx), 1000)\n",
    "    f = fn(dom)\n",
    "    mins = argrelmin(f)\n",
    "\n",
    "    closest_min = -1\n",
    "    closest_dist = -1\n",
    "    for m in np.nditer(mins):\n",
    "        dist = np.abs(500 - m)\n",
    "        if closest_min == -1 or closest_dist > dist:\n",
    "            closest_min = m\n",
    "            closest_dist = dist\n",
    "\n",
    "    fnprime = fn.derivative()\n",
    "    restrict = dom[np.max(closest_min-THRESHOLD_AREA, 0):\n",
    "                   closest_min+THRESHOLD_AREA]\n",
    "    f2 = fnprime(restrict)\n",
    "\n",
    "    m1 = restrict[np.argmax(f2)]\n",
    "    mean = fn(m1)\n",
    "\n",
    "    idx = np.min([int(np.floor(m1))+1, len(trim_coords)-1])\n",
    "    return (mean, trim_coords, idx)\n",
    "\n",
    "\n",
    "def get_closest_slice(rois):\n",
    "    ctrd = get_centroid(rois)\n",
    "    closest_slice = int(np.round(ctrd[0]))\n",
    "    return closest_slice\n",
    "\n",
    "\n",
    "def locate_lv_blood_pool(images, rois, circles, closest_slice, time):\n",
    "    best, best_fn = find_best_angle(rois[closest_slice],\n",
    "                                    circles[closest_slice])\n",
    "    mean, coords, idx = find_threshold_point(best, best_fn)\n",
    "    thresh, img_bin = cv2.threshold(images[closest_slice,\n",
    "                                           time].astype(np.float32),\n",
    "                                    mean, 255.0, cv2.THRESH_BINARY)\n",
    "    labeled, num = label(img_bin)\n",
    "    x, y = coords[idx]\n",
    "\n",
    "    count = 0\n",
    "    # Look along the line for a component. If one isn't found within a certain\n",
    "    # number of indices, just spit out the original coordinate.\n",
    "    while labeled[y][x] == 0:\n",
    "        idx += 1\n",
    "        count += 1\n",
    "        x, y = coords[idx]\n",
    "        if count > COMPONENT_INDEX_TOLERANCE:\n",
    "            idx -= count\n",
    "            x, y = coords[idx]\n",
    "            break\n",
    "\n",
    "    if count <= COMPONENT_INDEX_TOLERANCE:\n",
    "        component = np.transpose(np.nonzero(labeled == labeled[y][x]))\n",
    "    else:\n",
    "        component = np.array([[y, x]])\n",
    "\n",
    "    hull = cv2.convexHull(component)\n",
    "    squeezed = hull\n",
    "    if count <= COMPONENT_INDEX_TOLERANCE:\n",
    "        squeezed = np.squeeze(squeezed)\n",
    "    hull = np.fliplr(squeezed)\n",
    "\n",
    "    mask = np.zeros_like(labeled)\n",
    "    cv2.drawContours(mask, [hull], 0, 255, thickness=-1)\n",
    "    return mask, mean\n",
    "\n",
    "\n",
    "def propagate_segments(images, rois, base_mask, mean, closest_slice, time):\n",
    "    def propagate_segment(i, mask):\n",
    "        thresh, img_bin = cv2.threshold(images[i,\n",
    "                                               time].astype(np.float32),\n",
    "                                        mean, 255.0, cv2.THRESH_BINARY)\n",
    "\n",
    "        labeled, features = label(img_bin)\n",
    "\n",
    "        region1 = mask == 255\n",
    "        max_similar = -1\n",
    "        max_region = 0\n",
    "        for j in range(1, features+1):\n",
    "            region2 = labeled == j\n",
    "            intersect = np.count_nonzero(np.logical_and(region1, region2))\n",
    "            union = np.count_nonzero(np.logical_or(region1, region2))\n",
    "            similar = float(intersect) / union\n",
    "            if max_similar == -1 or max_similar < similar:\n",
    "                max_similar = similar\n",
    "                max_region = j\n",
    "\n",
    "        if max_similar == 0:\n",
    "            component = np.transpose(np.nonzero(mask))\n",
    "        else:\n",
    "            component = np.transpose(np.nonzero(labeled == max_region))\n",
    "        hull = cv2.convexHull(component)\n",
    "        hull = np.squeeze(hull)\n",
    "        if hull.shape == (2L,):\n",
    "            hull = np.array([hull])\n",
    "        hull = np.fliplr(hull)\n",
    "\n",
    "        newmask = np.zeros_like(img_bin)\n",
    "\n",
    "        cv2.drawContours(newmask, [hull], 0, 255, thickness=-1)\n",
    "\n",
    "        return newmask\n",
    "\n",
    "    (rois_depth, _, _) = rois.shape\n",
    "    newmask = base_mask\n",
    "    masks = {}\n",
    "    areas = {}\n",
    "    masks[closest_slice] = base_mask\n",
    "    areas[closest_slice] = np.count_nonzero(base_mask)\n",
    "    for i in range(closest_slice-1, -1, -1):\n",
    "        newmask = propagate_segment(i, newmask)\n",
    "        masks[i] = newmask\n",
    "        areas[i] = np.count_nonzero(newmask)\n",
    "\n",
    "    newmask = base_mask\n",
    "    for i in range(closest_slice+1, rois_depth):\n",
    "        newmask = propagate_segment(i, newmask)\n",
    "        masks[i] = newmask\n",
    "        areas[i] = np.count_nonzero(newmask)\n",
    "\n",
    "    return masks, areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(msg, lvl):\n",
    "    string = \"\"\n",
    "    for i in range(lvl):\n",
    "        string += \" \"\n",
    "    string += msg\n",
    "    print string\n",
    "\n",
    "\n",
    "def auto_segment_all_datasets(dir, num_samples):\n",
    "    studies = next(os.walk(os.path.join(dir, \"train\")))[1] + next(\n",
    "        os.walk(os.path.join(dir, \"validate\")))[1]\n",
    "    \n",
    "    studies = random.sample(studies, num_samples)\n",
    "    \n",
    "    if os.path.exists(\"output\"):\n",
    "        shutil.rmtree(\"output\")\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "    for s in studies:\n",
    "        if int(s) <= 500:\n",
    "            full_path = os.path.join(dir, \"train\", s)\n",
    "        else:\n",
    "            full_path = os.path.join(dir, \"validate\", s)\n",
    "\n",
    "        dset = Dataset(full_path, s)\n",
    "        print \"Processing dataset %s...\" % dset.name\n",
    "        try:\n",
    "            dset.load()\n",
    "            segment_dataset(dset)\n",
    "        except Exception as e:\n",
    "            log(\"***ERROR***: Exception %s thrown by dataset %s\" % (str(e), dset.name), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "\n",
    "We provide the input directory which has the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1...\n",
      " Calculating rois...\n",
      "  Calculating mean...\n",
      "  Performing Fourier transforms...\n",
      "   Fourier transforming on slice 0...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 0...\n",
      "   Performing Gaussian blur on slice 0...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 1...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 1...\n",
      "   Performing Gaussian blur on slice 1...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 2...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 2...\n",
      "   Performing Gaussian blur on slice 2...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 3...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 3...\n",
      "   Performing Gaussian blur on slice 3...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 4...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 4...\n",
      "   Performing Gaussian blur on slice 4...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 5...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 5...\n",
      "   Performing Gaussian blur on slice 5...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 6...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 6...\n",
      "   Performing Gaussian blur on slice 6...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 7...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 7...\n",
      "   Performing Gaussian blur on slice 7...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 8...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 8...\n",
      "   Performing Gaussian blur on slice 8...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 9...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 9...\n",
      "   Performing Gaussian blur on slice 9...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 10...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 10...\n",
      "   Performing Gaussian blur on slice 10...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 11...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 11...\n",
      "   Performing Gaussian blur on slice 11...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 12...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 12...\n",
      "   Performing Gaussian blur on slice 12...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 13...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 13...\n",
      "   Performing Gaussian blur on slice 13...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 14...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 14...\n",
      "   Performing Gaussian blur on slice 14...\n",
      "result(256, 230)\n",
      "   Fourier transforming on slice 15...\n",
      "ff(30, 256, 230)\n",
      "first_harmonic(256, 230)\n",
      "   Inverse Fourier transforming on slice 15...\n",
      "   Performing Gaussian blur on slice 15...\n",
      "result(256, 230)\n",
      "h1s(16, 256, 230)\n",
      "  Applying regression filter...\n",
      "   Beginning iteration 0 of regression...\n",
      "c1[   7.18965387  152.65675357  118.28857371]\n",
      "   Beginning iteration 1 of regression...\n",
      "c1[   6.89072514  142.3943858   113.27617336]\n",
      "   Beginning iteration 2 of regression...\n",
      "c1[   6.72647703  138.48988148  110.76135571]\n",
      "   Beginning iteration 3 of regression...\n",
      "c1[   6.63062534  136.30657583  109.74677786]\n",
      "   Beginning iteration 4 of regression...\n",
      "c1[   6.56120792  135.63593205  108.74603925]\n",
      "  Post-processing filtered images...\n",
      "   Performing final centroid regression...\n",
      "   Final image filtering...\n",
      "    Filtering image 1 of 16...\n",
      "    Filtering image 2 of 16...\n",
      "    Filtering image 3 of 16...\n",
      "    Filtering image 4 of 16...\n",
      "    Filtering image 5 of 16...\n",
      "    Filtering image 6 of 16...\n",
      "    Filtering image 7 of 16...\n",
      "    Filtering image 8 of 16...\n",
      "    Filtering image 9 of 16...\n",
      "    Filtering image 10 of 16...\n",
      "    Filtering image 11 of 16...\n",
      "    Filtering image 12 of 16...\n",
      "    Filtering image 13 of 16...\n",
      "    Filtering image 14 of 16...\n",
      "    Filtering image 15 of 16...\n",
      "    Filtering image 16 of 16...\n",
      "  Determining ROIs...\n",
      "   Getting ROI in slice 0...\n",
      "   Getting ROI in slice 1...\n",
      "   Getting ROI in slice 2...\n",
      "   Getting ROI in slice 3...\n",
      "   Getting ROI in slice 4...\n",
      "   Getting ROI in slice 5...\n",
      "   Getting ROI in slice 6...\n",
      "   Getting ROI in slice 7...\n",
      "   Getting ROI in slice 8...\n",
      "   Getting ROI in slice 9...\n",
      "   Getting ROI in slice 10...\n",
      "   Getting ROI in slice 11...\n",
      "   Getting ROI in slice 12...\n",
      "   Getting ROI in slice 13...\n",
      "   Getting ROI in slice 14...\n",
      "   Getting ROI in slice 15...\n",
      " Calculating areas...\n",
      "  Calculating areas at time 0...\n",
      "  Calculating areas at time 1...\n",
      "  Calculating areas at time 2...\n",
      "  Calculating areas at time 3...\n",
      "  Calculating areas at time 4...\n",
      "  Calculating areas at time 5...\n",
      "  Calculating areas at time 6...\n",
      "  Calculating areas at time 7...\n",
      "  Calculating areas at time 8...\n",
      "  Calculating areas at time 9...\n",
      "  Calculating areas at time 10...\n",
      "  Calculating areas at time 11...\n",
      "  Calculating areas at time 12...\n",
      "  Calculating areas at time 13...\n",
      "  Calculating areas at time 14...\n",
      "  Calculating areas at time 15...\n",
      "  Calculating areas at time 16...\n",
      "  Calculating areas at time 17...\n",
      "  Calculating areas at time 18...\n",
      "  Calculating areas at time 19...\n",
      "  Calculating areas at time 20...\n",
      "  Calculating areas at time 21...\n",
      "  Calculating areas at time 22...\n",
      "  Calculating areas at time 23...\n",
      "  Calculating areas at time 24...\n",
      "  Calculating areas at time 25...\n",
      "  Calculating areas at time 26...\n",
      "  Calculating areas at time 27...\n",
      "  Calculating areas at time 28...\n",
      "  Calculating areas at time 29...\n",
      " Calculating volumes...\n",
      " Calculating ef...\n",
      " Done, ef is 0.718763\n",
      "# of images: 30, # of labels: 30\n",
      "# of images: 60, # of labels: 60\n",
      "# of images: 90, # of labels: 90\n",
      "# of images: 120, # of labels: 120\n",
      "# of images: 150, # of labels: 150\n",
      "# of images: 180, # of labels: 180\n",
      "# of images: 210, # of labels: 210\n",
      "# of images: 240, # of labels: 240\n",
      "# of images: 270, # of labels: 270\n",
      "# of images: 300, # of labels: 300\n",
      "# of images: 330, # of labels: 330\n",
      "# of images: 360, # of labels: 360\n",
      "# of images: 390, # of labels: 390\n",
      "# of images: 420, # of labels: 420\n",
      "# of images: 450, # of labels: 450\n",
      "# of images: 480, # of labels: 480\n",
      "Processing 480 images and labels...\n",
      "Processed 100 images\n",
      "Processed 100 labels\n",
      "Processed 200 images\n",
      "Processed 200 labels\n",
      "Processed 300 images\n",
      "Processed 300 labels\n",
      "Processed 400 images\n",
      "Processed 400 labels\n",
      "Processed 480 images\n",
      "Processed 480 labels\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./data\"\n",
    "num_samples = 1\n",
    "\n",
    "auto_segment_all_datasets(input_dir, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
