{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages\n",
    "\n",
    "We use pydicom, opencv and lmdb for reading and interpreting DICOM files from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all packages\n"
     ]
    }
   ],
   "source": [
    "import cv2, dicom, lmdb, json\n",
    "import os, random, re, sys, shutil\n",
    "import fnmatch, subprocess\n",
    "from IPython.utils import io\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.ndimage.morphology import binary_erosion\n",
    "from scipy.fftpack import fftn, ifftn\n",
    "from scipy.signal import argrelmin, correlate\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
    "\n",
    "CAFFE_ROOT = \"/home/ubuntu/caffe\"\n",
    "caffe_path = os.path.join(CAFFE_ROOT, \"python\")\n",
    "if caffe_path not in sys.path:\n",
    "    sys.path.insert(0, caffe_path)\n",
    "\n",
    "import caffe\n",
    "\n",
    "print \"Successfully imported all packages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined all parameters\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# PARAMETERS\n",
    "#\n",
    "\n",
    "\n",
    "# number of bins to use in histogram for gaussian regression\n",
    "NUM_BINS = 100\n",
    "# number of standard deviations past which we will consider a pixel an outlier\n",
    "STD_MULTIPLIER = 2\n",
    "# number of points of our interpolated dataset to consider when searching for\n",
    "# a threshold value; the function by default is interpolated over 1000 points,\n",
    "# so 250 will look at the half of the points that is centered around the known\n",
    "# myocardium pixel\n",
    "THRESHOLD_AREA = 250\n",
    "# number of pixels on the line within which to search for a connected component\n",
    "# in a thresholded image, increase this to look for components further away\n",
    "COMPONENT_INDEX_TOLERANCE = 20\n",
    "# number of angles to search when looking for the correct orientation\n",
    "ANGLE_SLICES = 36\n",
    "\n",
    "print \"Defined all parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "\n",
    "This class traverses the input data and reads the input data set (DICOM) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    dataset_count = 0\n",
    "\n",
    "    def __init__(self, directory, subdir):\n",
    "        # deal with any intervening directories\n",
    "        while True:\n",
    "            subdirs = next(os.walk(directory))[1]\n",
    "            if len(subdirs) == 1:\n",
    "                directory = os.path.join(directory, subdirs[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        slices = []\n",
    "        for s in subdirs:\n",
    "            m = re.match(\"sax_(\\d+)\", s)\n",
    "            if m is not None:\n",
    "                slices.append(int(m.group(1)))\n",
    "\n",
    "        slices_map = {}\n",
    "        first = True\n",
    "        times = []\n",
    "        for s in slices:\n",
    "            files = next(os.walk(os.path.join(directory, \"sax_%d\" % s)))[2]\n",
    "            offset = None\n",
    "\n",
    "            for f in files:\n",
    "                m = re.match(\"IM-(\\d{4,})-(\\d{4})\\.dcm\", f)\n",
    "                if m is not None:\n",
    "                    if first:\n",
    "                        times.append(int(m.group(2)))\n",
    "                    if offset is None:\n",
    "                        offset = int(m.group(1))\n",
    "\n",
    "            first = False\n",
    "            slices_map[s] = offset\n",
    "\n",
    "        self.directory = directory\n",
    "        self.time = sorted(times)\n",
    "        self.slices = sorted(slices)\n",
    "        self.slices_map = slices_map\n",
    "        Dataset.dataset_count += 1\n",
    "        self.name = subdir\n",
    "\n",
    "    def _filename(self, s, t):\n",
    "        return os.path.join(self.directory,\"sax_%d\" % s, \"IM-%04d-%04d.dcm\" % (self.slices_map[s], t))\n",
    "\n",
    "    def _read_dicom_image(self, filename):\n",
    "        d = dicom.read_file(filename)\n",
    "        img = d.pixel_array\n",
    "        return np.array(img)\n",
    "\n",
    "    def _read_all_dicom_images(self):\n",
    "        f1 = self._filename(self.slices[0], self.time[0])\n",
    "        d1 = dicom.read_file(f1)\n",
    "        (x, y) = d1.PixelSpacing\n",
    "        (x, y) = (float(x), float(y))\n",
    "        f2 = self._filename(self.slices[1], self.time[0])\n",
    "        d2 = dicom.read_file(f2)\n",
    "\n",
    "        # try a couple of things to measure distance between slices\n",
    "        try:\n",
    "            dist = np.abs(d2.SliceLocation - d1.SliceLocation)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                dist = d1.SliceThickness\n",
    "            except AttributeError:\n",
    "                dist = 8  # better than nothing...\n",
    "\n",
    "        self.images = np.array([[self._read_dicom_image(self._filename(d, i))\n",
    "                                 for i in self.time]\n",
    "                                for d in self.slices])\n",
    "        self.dist = dist\n",
    "        self.area_multiplier = x * y\n",
    "\n",
    "    def load(self):\n",
    "        self._read_all_dicom_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumes dataset is loaded, call dataset.load()\n",
    "def segment_dataset(dataset):\n",
    "    images = dataset.images\n",
    "    dist = dataset.dist\n",
    "    areaMultiplier = dataset.area_multiplier\n",
    "    # shape: num slices, num snapshots, rows, columns\n",
    "    log(\"Calculating rois...\", 1)\n",
    "    rois, circles = calc_rois(images)\n",
    "    log(\"Calculating areas...\", 1)\n",
    "    all_masks, all_areas = calc_all_areas(images, rois, circles)\n",
    "    log(\"Calculating volumes...\", 1)\n",
    "    area_totals = [calc_total_volume(a, areaMultiplier, dist)\n",
    "                   for a in all_areas]\n",
    "    log(\"Calculating ef...\", 1)\n",
    "    edv = max(area_totals)\n",
    "    esv = min(area_totals)\n",
    "    ef = (edv - esv) / edv\n",
    "    log(\"Done, ef is %f\" % ef, 1)\n",
    "\n",
    "    save_masks_to_dir(dataset, all_masks)\n",
    "\n",
    "    output = {}\n",
    "    output[\"edv\"] = edv\n",
    "    output[\"esv\"] = esv\n",
    "    output[\"ef\"] = ef\n",
    "    output[\"areas\"] = all_areas.tolist()\n",
    "    f = open(\"output/%s/output.json\" % dataset.name, \"w\")\n",
    "    json.dump(output, f, indent=2)\n",
    "    f.close()\n",
    "    dataset.edv = edv\n",
    "    dataset.esv = esv\n",
    "    dataset.ef = ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_masks_to_dir(dataset, all_masks):\n",
    "    imgs, labels = [], []\n",
    "    os.mkdir(\"output/%s\" % dataset.name)\n",
    "    if os.path.exists(\"input\"):\n",
    "        shutil.rmtree(\"input\")\n",
    "    os.mkdir(\"input\")\n",
    "    os.mkdir(\"input/%s\" % dataset.name)\n",
    "    for s in range(len(dataset.slices)):\n",
    "        os.mkdir(\"output/%s/slice%02d\" % (dataset.name, s))\n",
    "        os.mkdir(\"input/%s/slice%02d\" % (dataset.name, s))\n",
    "        for t in range(len(dataset.time)):\n",
    "            mask = all_masks[t][s]\n",
    "            image.imsave(\"output/%s/slice%02d/time%02d_mask.png\" %\n",
    "                         (dataset.name, s, t), mask)\n",
    "            labels.append(mask)\n",
    "            image_save = dataset.images[s][t]\n",
    "            image.imsave(\"input/%s/slice%02d/time%02d_DICOM.png\" %\n",
    "                         (dataset.name, s, t), image_save)\n",
    "            imgs.append(image_save)\n",
    "            \n",
    "        print \"# of images: {0}, # of labels: {1}\".format(len(imgs), len(labels))\n",
    "\n",
    "    for lmdb_name in [\"lmdb_img_name\", \"lmdb_label_name\"]:\n",
    "        db_path = os.path.abspath(lmdb_name)\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "    counter_img = 0\n",
    "    counter_label = 0\n",
    "    batchsz = 100\n",
    "    print(\"Processing {:d} images and labels...\".format(len(imgs)))\n",
    "    for i in xrange(int(np.ceil(len(imgs) / float(batchsz)))):\n",
    "        batch_imgs = imgs[(batchsz*i):(batchsz*(i+1))]\n",
    "        batch_labels = labels[(batchsz*i):(batchsz*(i+1))]\n",
    "        if len(batch_imgs) == 0:\n",
    "            break\n",
    "        db_imgs = lmdb.open(\"lmdb_img_name\", map_size=1e12)\n",
    "        with db_imgs.begin(write=True) as txn_img:          \n",
    "            for img in batch_imgs:\n",
    "                print \"BP 3\"\n",
    "                print(type(img))\n",
    "                datum = caffe.io.array_to_datum(np.expand_dims(img, axis=0))\n",
    "                print \"BP 4\"\n",
    "                txn_img.put(\"{:0>10d}\".format(counter_img), datum.SerializeToString())\n",
    "                print \"BP 5\"\n",
    "                counter_img += 1\n",
    "        print(\"Processed {:d} images\".format(counter_img))\n",
    "        db_labels = lmdb.open(\"lmdb_label_name\", map_size=1e12)\n",
    "        with db_labels.begin(write=True) as txn_label:\n",
    "            for lbl in batch_labels:\n",
    "                print \"BP 6\"\n",
    "                datum = caffe.io.array_to_datum(np.expand_dims(lbl, axis=0))\n",
    "                txn_label.put(\"{:0>10d}\".format(counter_label), datum.SerializeToString())\n",
    "                counter_label += 1\n",
    "        print(\"Processed {:d} labels\".format(counter_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_rois(images):\n",
    "    (num_slices, _, _, _) = images.shape\n",
    "    log(\"Calculating mean...\", 2)\n",
    "    dc = np.mean(images, 1)\n",
    "\n",
    "    def get_H1(i):\n",
    "        log(\"Fourier transforming on slice %d...\" % i, 3)\n",
    "        ff = fftn(images[i])\n",
    "        print \"ff\"+str(np.shape(ff))\n",
    "        first_harmonic = ff[1, :, :]\n",
    "        print \"first_harmonic\"+str(np.shape(first_harmonic))\n",
    "        log(\"Inverse Fourier transforming on slice %d...\" % i, 3)\n",
    "        result = np.absolute(ifftn(first_harmonic))\n",
    "        log(\"Performing Gaussian blur on slice %d...\" % i, 3)\n",
    "        result = cv2.GaussianBlur(result, (5, 5), 0)\n",
    "        print \"result\" + str(np.shape(result))\n",
    "        return result\n",
    "\n",
    "    log(\"Performing Fourier transforms...\", 2)\n",
    "    h1s = np.array([get_H1(i) for i in range(num_slices)])\n",
    "    print \"h1s\" + str(np.shape(h1s))\n",
    "    m = np.max(h1s) * 0.05\n",
    "    h1s[h1s < m] = 0\n",
    "\n",
    "    log(\"Applying regression filter...\", 2)\n",
    "    regress_h1s = regression_filter(h1s)\n",
    "    log(\"Post-processing filtered images...\", 2)\n",
    "    proc_regress_h1s, coords = post_process_regression(regress_h1s)\n",
    "    log(\"Determining ROIs...\", 2)\n",
    "    rois, circles = get_ROIs(dc, proc_regress_h1s, coords)\n",
    "    return rois, circles\n",
    "\n",
    "\n",
    "def calc_all_areas(images, rois, circles):\n",
    "    closest_slice = get_closest_slice(rois)\n",
    "    (_, times, _, _) = images.shape\n",
    "\n",
    "    def calc_areas(time):\n",
    "        log(\"Calculating areas at time %d...\" % time, 2)\n",
    "        mask, mean = locate_lv_blood_pool(images, rois, circles, closest_slice,\n",
    "                                          time)\n",
    "        masks, areas = propagate_segments(images, rois, mask, mean,\n",
    "                                          closest_slice, time)\n",
    "        return (masks, areas)\n",
    "\n",
    "    result = np.transpose(map(calc_areas, range(times)))\n",
    "    all_masks = result[0]\n",
    "    all_areas = result[1]\n",
    "    return all_masks, all_areas\n",
    "\n",
    "\n",
    "def calc_total_volume(areas, area_multiplier, dist):\n",
    "    slices = np.array(sorted(areas.keys()))\n",
    "    modified = [areas[i] * area_multiplier for i in slices]\n",
    "    vol = 0\n",
    "    for i in slices[:-1]:\n",
    "        a, b = modified[i], modified[i+1]\n",
    "        subvol = (dist/3.0) * (a + np.sqrt(a*b) + b)\n",
    "        vol += subvol / 1000.0  # conversion to mL\n",
    "    return vol\n",
    "\n",
    "\n",
    "def get_centroid(img):\n",
    "    nz = np.nonzero(img)\n",
    "    pxls = np.transpose(nz)\n",
    "    weights = img[nz]\n",
    "    avg = np.average(pxls, axis=0, weights=weights)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def regress_centroids(cs):\n",
    "    num_slices = len(cs)\n",
    "    y_centroids = cs[:, 0]\n",
    "    x_centroids = cs[:, 1]\n",
    "    z_values = np.array(range(num_slices))\n",
    "\n",
    "    (xslope, xintercept, _, _, _) = linregress(z_values, x_centroids)\n",
    "    (yslope, yintercept, _, _, _) = linregress(z_values, y_centroids)\n",
    "\n",
    "    return (xslope, xintercept, yslope, yintercept)\n",
    "\n",
    "\n",
    "def get_weighted_distances(imgs, coords, xs, xi, ys, yi):\n",
    "    a = np.array([0, yi, xi])\n",
    "    n = np.array([1, ys, xs])\n",
    "\n",
    "    zeros = np.zeros(3)\n",
    "\n",
    "    def dist(p):\n",
    "        to_line = (a - p) - (np.dot((a - p), n) * n)\n",
    "        d = euclidean(zeros, to_line)\n",
    "        return d\n",
    "\n",
    "    def weight(p):\n",
    "        (z, y, x) = p\n",
    "        return imgs[z, y, x]\n",
    "\n",
    "    dists = np.array([dist(c) for c in coords])\n",
    "    weights = np.array([weight(c) for c in coords])\n",
    "    return (coords, dists, weights)\n",
    "\n",
    "\n",
    "def gaussian_fit(dists, weights):\n",
    "    # based on http://stackoverflow.com/questions/11507028/fit-a-gaussian-function\n",
    "    (x, y) = histogram_transform(dists, weights)\n",
    "    fivep = int(len(x) * 0.05)\n",
    "    xtmp = x\n",
    "    ytmp = y\n",
    "    fromFront = False\n",
    "    while True:\n",
    "        if len(xtmp) == 0 and len(ytmp) == 0:\n",
    "            if fromFront:\n",
    "                # well we failed\n",
    "                idx = np.argmax(y)\n",
    "                xmax = x[idx]\n",
    "                p0 = [max(y), xmax, xmax]\n",
    "                (A, mu, sigma) = p0\n",
    "                return mu, sigma, lambda x: gauss(x, A, mu, sigma)\n",
    "            else:\n",
    "                fromFront = True\n",
    "                xtmp = x\n",
    "                ytmp = y\n",
    "\n",
    "        idx = np.argmax(ytmp)\n",
    "        xmax = xtmp[idx]\n",
    "\n",
    "        def gauss(x, *p):\n",
    "            A, mu, sigma = p\n",
    "            return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "        p0 = [max(ytmp), xmax, xmax]\n",
    "        try:\n",
    "            coeff, var_matrix = curve_fit(gauss, xtmp, ytmp, p0=p0)\n",
    "            (A, mu, sigma) = coeff\n",
    "            return (mu, sigma, lambda x: gauss(x, A, mu, sigma))\n",
    "        except RuntimeError:\n",
    "            if fromFront:\n",
    "                xtmp = xtmp[fivep:]\n",
    "                ytmp = ytmp[fivep:]\n",
    "            else:\n",
    "                xtmp = xtmp[:-fivep]\n",
    "                ytmp = ytmp[:-fivep]\n",
    "\n",
    "\n",
    "def histogram_transform(values, weights):\n",
    "    hist, bins = np.histogram(values, bins=NUM_BINS, weights=weights)\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    bin_centers = bins[:-1] + (bin_width / 2)\n",
    "\n",
    "    return (bin_centers, hist)\n",
    "\n",
    "\n",
    "def get_outliers(coords, dists, weights):\n",
    "    fivep = int(len(weights) * 0.05)\n",
    "    ctr = 1\n",
    "    while True:\n",
    "        (mean, std, fn) = gaussian_fit(dists, weights)\n",
    "        low_values = dists < (mean - STD_MULTIPLIER*np.abs(std))\n",
    "        high_values = dists > (mean + STD_MULTIPLIER*np.abs(std))\n",
    "        outliers = np.logical_or(low_values, high_values)\n",
    "        if len(coords[outliers]) == len(coords):\n",
    "            weights[-fivep*ctr:] = 0\n",
    "            ctr += 1\n",
    "        else:\n",
    "            return coords[outliers]\n",
    "\n",
    "\n",
    "def regress_and_filter_distant(imgs):\n",
    "    centroids = np.array([get_centroid(img) for img in imgs])\n",
    "    raw_coords = np.transpose(np.nonzero(imgs))\n",
    "    (xslope, xintercept, yslope, yintercept) = regress_centroids(centroids)\n",
    "    (coords, dists, weights) = get_weighted_distances(imgs, raw_coords, xslope,\n",
    "                                                      xintercept, yslope,\n",
    "                                                      yintercept)\n",
    "    outliers = get_outliers(coords, dists, weights)\n",
    "    imgs_cpy = np.copy(imgs)\n",
    "    for c in outliers:\n",
    "        (z, x, y) = c\n",
    "        imgs_cpy[z, x, y] = 0\n",
    "    return imgs_cpy\n",
    "\n",
    "\n",
    "def regression_filter(imgs):\n",
    "    condition = True\n",
    "    iternum = 0\n",
    "    while(condition):\n",
    "        log(\"Beginning iteration %d of regression...\" % iternum, 3)\n",
    "        iternum += 1\n",
    "        imgs_filtered = regress_and_filter_distant(imgs)\n",
    "        c1 = get_centroid(imgs)\n",
    "        print \"c1\" + str(c1)\n",
    "        c2 = get_centroid(imgs_filtered)\n",
    "        dc = np.linalg.norm(c1 - c2)\n",
    "        imgs = imgs_filtered\n",
    "        condition = (dc > 1.0)  # because python has no do-while loops\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def post_process_regression(imgs):\n",
    "    (numimgs, _, _) = imgs.shape\n",
    "    centroids = np.array([get_centroid(img) for img in imgs])\n",
    "    log(\"Performing final centroid regression...\", 3)\n",
    "    (xslope, xintercept, yslope, yintercept) = regress_centroids(centroids)\n",
    "    imgs_cpy = np.copy(imgs)\n",
    "\n",
    "    def filter_one_img(zlvl):\n",
    "        points_on_zlvl = np.transpose(imgs[zlvl].nonzero())\n",
    "        points_on_zlvl = np.insert(points_on_zlvl, 0, zlvl, axis=1)\n",
    "        (coords, dists, weights) = get_weighted_distances(imgs, points_on_zlvl,\n",
    "                                                          xslope, xintercept,\n",
    "                                                          yslope, yintercept)\n",
    "        outliers = get_outliers(coords, dists, weights)\n",
    "        for c in outliers:\n",
    "            (z, x, y) = c\n",
    "            imgs_cpy[z, x, y] = 0\n",
    "\n",
    "    log(\"Final image filtering...\", 3)\n",
    "    for z in range(numimgs):\n",
    "        log(\"Filtering image %d of %d...\" % (z+1, numimgs), 4)\n",
    "        filter_one_img(z)\n",
    "\n",
    "    return (imgs_cpy, (xslope, xintercept, yslope, yintercept))\n",
    "\n",
    "\n",
    "def floats_draw_circle(img, center, r, color, thickness):\n",
    "    (x, y) = center\n",
    "    x, y = int(np.round(x)), int(np.round(y))\n",
    "    r = int(np.round(r))\n",
    "    cv2.circle(img, center=(x, y), radius=r, color=color, thickness=thickness)\n",
    "\n",
    "\n",
    "def filled_ratio_of_circle(img, center, r):\n",
    "    mask = np.zeros_like(img)\n",
    "    floats_draw_circle(mask, center, r, 1, -1)\n",
    "    masked = mask * img\n",
    "    (x, _) = np.nonzero(mask)\n",
    "    (x2, _) = np.nonzero(masked)\n",
    "    if x.size == 0:\n",
    "        return 0\n",
    "    return float(x2.size) / x.size\n",
    "\n",
    "\n",
    "def circle_smart_radius(img, center):\n",
    "    domain = np.arange(1, 100)\n",
    "    (xintercept, yintercept) = center\n",
    "\n",
    "    def ratio(r):\n",
    "        return filled_ratio_of_circle(img, (xintercept, yintercept), r)*r\n",
    "\n",
    "    y = np.array([ratio(d) for d in domain])\n",
    "    most = np.argmax(y)\n",
    "    return domain[most]\n",
    "\n",
    "\n",
    "def get_ROIs(originals, h1s, regression_params):\n",
    "    (xslope, xintercept, yslope, yintercept) = regression_params\n",
    "    (num_slices, _, _) = h1s.shape\n",
    "    results = []\n",
    "    circles = []\n",
    "    for i in range(num_slices):\n",
    "        log(\"Getting ROI in slice %d...\" % i, 3)\n",
    "        o = originals[i]\n",
    "        h = h1s[i]\n",
    "        ctr = (xintercept + xslope * i, yintercept + yslope * i)\n",
    "        r = circle_smart_radius(h, ctr)\n",
    "        tmp = np.zeros_like(o)\n",
    "        floats_draw_circle(tmp, ctr, r, 1, -1)\n",
    "        results.append(tmp * o)\n",
    "        circles.append((ctr, r))\n",
    "\n",
    "    return (np.array(results), np.array(circles))\n",
    "\n",
    "\n",
    "def bresenham(x0, x1, y0, y1, fn):\n",
    "    # using some pseudocode from\n",
    "    # https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm\n",
    "    # and also https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm\n",
    "    steep = abs(y1-y0) > abs(x1-x0)\n",
    "    if steep:\n",
    "        x0, x1, y0, y1 = y0, y1, x0, x1\n",
    "    if x0 > x1:\n",
    "        x0, x1, y0, y1 = x1, x0, y1, y0\n",
    "\n",
    "    def plot(x, y):\n",
    "        if steep:\n",
    "            fn(y, x)\n",
    "        else:\n",
    "            fn(x, y)\n",
    "\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "\n",
    "    D = 2*np.abs(dy) - dx\n",
    "    plot(x0, y0)\n",
    "    y = y0\n",
    "\n",
    "    for x in range(x0+1, x1+1):  # x0+1 to x1\n",
    "        D = D + 2*np.abs(dy)\n",
    "        if D > 0:\n",
    "            y += np.sign(dy)\n",
    "            D -= 2*dx\n",
    "        plot(x, y)\n",
    "\n",
    "\n",
    "def line_thru(bounds, center, theta):\n",
    "    (xmin, xmax, ymin, ymax) = bounds\n",
    "    (cx, cy) = center\n",
    "\n",
    "    if np.cos(theta) == 0:\n",
    "        return (cx, ymin, cx, ymax)\n",
    "    slope = np.tan(theta)\n",
    "\n",
    "    x0 = xmin\n",
    "    y0 = cy - (cx - xmin) * slope\n",
    "    if y0 < ymin:\n",
    "        y0 = ymin\n",
    "        x0 = max(xmin, cx - ((cy - ymin) / slope))\n",
    "    elif y0 > ymax:\n",
    "        y0 = ymax\n",
    "        x0 = max(xmin, cx - ((cy - ymax) / slope))\n",
    "\n",
    "    x1 = xmax\n",
    "    y1 = cy + (xmax - cx) * slope\n",
    "    if y1 < ymin:\n",
    "        y1 = ymin\n",
    "        x1 = min(xmax, cx + ((ymin - cy) / slope))\n",
    "    elif y1 > ymax:\n",
    "        y1 = ymax\n",
    "        x1 = min(xmax, cx + ((ymax - cy) / slope))\n",
    "\n",
    "    return (x0, x1, y0, y1)\n",
    "\n",
    "\n",
    "def get_line_coords(w, h, cx, cy, theta):\n",
    "    coords = np.floor(np.array(line_thru((0, w-1, 0, h-1), (cx, cy), theta)))\n",
    "    return coords.astype(np.int)\n",
    "\n",
    "\n",
    "def trim_zeros_indices(has_zeros):\n",
    "    first = 0\n",
    "    for i in has_zeros:\n",
    "        if i == 0:\n",
    "            first += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    last = len(has_zeros)\n",
    "    for i in has_zeros[::-1]:\n",
    "        if i == 0:\n",
    "            last -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return first, last\n",
    "\n",
    "\n",
    "def get_line(roi, cx, cy, theta):\n",
    "    (h, w) = roi.shape\n",
    "    (x0, x1, y0, y1) = get_line_coords(w, h, cx, cy, theta)\n",
    "\n",
    "    intensities = []\n",
    "    coords = []\n",
    "\n",
    "    def collect(x, y):\n",
    "        if y < 0 or y >= h or x < 0 or x >= w:\n",
    "            return\n",
    "        intensities.append(roi[y, x])\n",
    "        coords.append((x, y))\n",
    "\n",
    "    bresenham(x0, x1, y0, y1, collect)\n",
    "\n",
    "    def geti(idx):\n",
    "        return intensities[idx]\n",
    "\n",
    "    getiv = np.vectorize(geti)\n",
    "    x = np.arange(0, len(intensities))\n",
    "    y = getiv(x)\n",
    "    first, last = trim_zeros_indices(y)\n",
    "    trimy = y[first:last]\n",
    "    trimcoords = coords[first:last]\n",
    "\n",
    "    trimx = np.arange(0, trimy.size)\n",
    "    return (trimx, trimy, trimcoords)\n",
    "\n",
    "\n",
    "def find_best_angle(roi, circ):\n",
    "    ((cx, cy), r) = circ\n",
    "    results = np.zeros(ANGLE_SLICES)\n",
    "    fns = [None for i in range(ANGLE_SLICES)]\n",
    "\n",
    "    COS_MATCHED_FILTER_FREQ = 2.5\n",
    "\n",
    "    def score_matched(trimx, trimy):\n",
    "        # first, normalize this data\n",
    "        newtrimx = np.linspace(0.0, 1.0, np.size(trimx))\n",
    "        minimum = np.min(trimy)\n",
    "        maximum = np.max(trimy) - minimum\n",
    "        newtrimy = (trimy - minimum) / maximum\n",
    "\n",
    "        filt = 1 - ((np.cos(COS_MATCHED_FILTER_FREQ*2*np.pi*newtrimx)) /\n",
    "                    2 + (0.5))\n",
    "        cr = correlate(newtrimy, filt, mode=\"same\")\n",
    "        return np.max(cr)\n",
    "\n",
    "    for i in range(ANGLE_SLICES):\n",
    "        trimx, trimy, trimcoords = get_line(roi, cx, cy, np.pi*i/ANGLE_SLICES)\n",
    "        score2 = score_matched(trimx, trimy)\n",
    "        results[i] = score2\n",
    "        fns[i] = (UnivariateSpline(trimx, trimy), trimx, trimcoords)\n",
    "\n",
    "    best = np.argmax(results)\n",
    "    return (best * np.pi / ANGLE_SLICES, fns[best])\n",
    "\n",
    "\n",
    "def find_threshold_point(best, best_fn):\n",
    "    fn, trimx, trim_coords = best_fn\n",
    "    dom = np.linspace(np.min(trimx), np.max(trimx), 1000)\n",
    "    f = fn(dom)\n",
    "    mins = argrelmin(f)\n",
    "\n",
    "    closest_min = -1\n",
    "    closest_dist = -1\n",
    "    for m in np.nditer(mins):\n",
    "        dist = np.abs(500 - m)\n",
    "        if closest_min == -1 or closest_dist > dist:\n",
    "            closest_min = m\n",
    "            closest_dist = dist\n",
    "\n",
    "    fnprime = fn.derivative()\n",
    "    restrict = dom[np.max(closest_min-THRESHOLD_AREA, 0):\n",
    "                   closest_min+THRESHOLD_AREA]\n",
    "    f2 = fnprime(restrict)\n",
    "\n",
    "    m1 = restrict[np.argmax(f2)]\n",
    "    mean = fn(m1)\n",
    "\n",
    "    idx = np.min([int(np.floor(m1))+1, len(trim_coords)-1])\n",
    "    return (mean, trim_coords, idx)\n",
    "\n",
    "\n",
    "def get_closest_slice(rois):\n",
    "    ctrd = get_centroid(rois)\n",
    "    closest_slice = int(np.round(ctrd[0]))\n",
    "    return closest_slice\n",
    "\n",
    "\n",
    "def locate_lv_blood_pool(images, rois, circles, closest_slice, time):\n",
    "    best, best_fn = find_best_angle(rois[closest_slice],\n",
    "                                    circles[closest_slice])\n",
    "    mean, coords, idx = find_threshold_point(best, best_fn)\n",
    "    thresh, img_bin = cv2.threshold(images[closest_slice,\n",
    "                                           time].astype(np.float32),\n",
    "                                    mean, 255.0, cv2.THRESH_BINARY)\n",
    "    labeled, num = label(img_bin)\n",
    "    x, y = coords[idx]\n",
    "\n",
    "    count = 0\n",
    "    # Look along the line for a component. If one isn't found within a certain\n",
    "    # number of indices, just spit out the original coordinate.\n",
    "    while labeled[y][x] == 0:\n",
    "        idx += 1\n",
    "        count += 1\n",
    "        x, y = coords[idx]\n",
    "        if count > COMPONENT_INDEX_TOLERANCE:\n",
    "            idx -= count\n",
    "            x, y = coords[idx]\n",
    "            break\n",
    "\n",
    "    if count <= COMPONENT_INDEX_TOLERANCE:\n",
    "        component = np.transpose(np.nonzero(labeled == labeled[y][x]))\n",
    "    else:\n",
    "        component = np.array([[y, x]])\n",
    "\n",
    "    hull = cv2.convexHull(component)\n",
    "    squeezed = hull\n",
    "    if count <= COMPONENT_INDEX_TOLERANCE:\n",
    "        squeezed = np.squeeze(squeezed)\n",
    "    hull = np.fliplr(squeezed)\n",
    "\n",
    "    mask = np.zeros_like(labeled)\n",
    "    cv2.drawContours(mask, [hull], 0, 255, thickness=-1)\n",
    "    return mask, mean\n",
    "\n",
    "\n",
    "def propagate_segments(images, rois, base_mask, mean, closest_slice, time):\n",
    "    def propagate_segment(i, mask):\n",
    "        thresh, img_bin = cv2.threshold(images[i,\n",
    "                                               time].astype(np.float32),\n",
    "                                        mean, 255.0, cv2.THRESH_BINARY)\n",
    "\n",
    "        labeled, features = label(img_bin)\n",
    "\n",
    "        region1 = mask == 255\n",
    "        max_similar = -1\n",
    "        max_region = 0\n",
    "        for j in range(1, features+1):\n",
    "            region2 = labeled == j\n",
    "            intersect = np.count_nonzero(np.logical_and(region1, region2))\n",
    "            union = np.count_nonzero(np.logical_or(region1, region2))\n",
    "            similar = float(intersect) / union\n",
    "            if max_similar == -1 or max_similar < similar:\n",
    "                max_similar = similar\n",
    "                max_region = j\n",
    "\n",
    "        if max_similar == 0:\n",
    "            component = np.transpose(np.nonzero(mask))\n",
    "        else:\n",
    "            component = np.transpose(np.nonzero(labeled == max_region))\n",
    "        hull = cv2.convexHull(component)\n",
    "        hull = np.squeeze(hull)\n",
    "        if hull.shape == (2L,):\n",
    "            hull = np.array([hull])\n",
    "        hull = np.fliplr(hull)\n",
    "\n",
    "        newmask = np.zeros_like(img_bin)\n",
    "\n",
    "        cv2.drawContours(newmask, [hull], 0, 255, thickness=-1)\n",
    "\n",
    "        return newmask\n",
    "\n",
    "    (rois_depth, _, _) = rois.shape\n",
    "    newmask = base_mask\n",
    "    masks = {}\n",
    "    areas = {}\n",
    "    masks[closest_slice] = base_mask\n",
    "    areas[closest_slice] = np.count_nonzero(base_mask)\n",
    "    for i in range(closest_slice-1, -1, -1):\n",
    "        newmask = propagate_segment(i, newmask)\n",
    "        masks[i] = newmask\n",
    "        areas[i] = np.count_nonzero(newmask)\n",
    "\n",
    "    newmask = base_mask\n",
    "    for i in range(closest_slice+1, rois_depth):\n",
    "        newmask = propagate_segment(i, newmask)\n",
    "        masks[i] = newmask\n",
    "        areas[i] = np.count_nonzero(newmask)\n",
    "\n",
    "    return masks, areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(msg, lvl):\n",
    "    string = \"\"\n",
    "    for i in range(lvl):\n",
    "        string += \" \"\n",
    "    string += msg\n",
    "    print string\n",
    "\n",
    "\n",
    "def auto_segment_all_datasets(dir):\n",
    "    studies = next(os.walk(os.path.join(dir, \"train\")))[1] + next(\n",
    "        os.walk(os.path.join(dir, \"validate\")))[1]\n",
    "\n",
    "    labels = np.loadtxt(os.path.join(dir, \"train.csv\"), delimiter=\",\",\n",
    "                        skiprows=1)\n",
    "\n",
    "    label_map = {}\n",
    "    for l in labels:\n",
    "        label_map[l[0]] = (l[2], l[1])\n",
    "\n",
    "    num_samples = None\n",
    "    if len(sys.argv) > 2:\n",
    "        num_samples = int(sys.argv[2])\n",
    "        studies = random.sample(studies, num_samples)\n",
    "    if os.path.exists(\"output\"):\n",
    "        shutil.rmtree(\"output\")\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "    accuracy_csv = open(\"accuracy.csv\", \"w\")\n",
    "    accuracy_csv.write(\"Dataset,Actual EDV,Actual ESV,Predicted EDV,\"\n",
    "                       \"Predicted ESV\\n\")\n",
    "    submit_csv = open(\"submit.csv\", \"w\")\n",
    "    submit_csv.write(\"Id,\")\n",
    "    for i in range(0, 600):\n",
    "        submit_csv.write(\"P%d\" % i)\n",
    "        if i != 599:\n",
    "            submit_csv.write(\",\")\n",
    "        else:\n",
    "            submit_csv.write(\"\\n\")\n",
    "\n",
    "    for s in studies:\n",
    "        if int(s) <= 500:\n",
    "            full_path = os.path.join(d, \"train\", s)\n",
    "        else:\n",
    "            full_path = os.path.join(d, \"validate\", s)\n",
    "\n",
    "        dset = Dataset(full_path, s)\n",
    "        print \"Processing dataset %s...\" % dset.name\n",
    "        p_edv = 0\n",
    "        p_esv = 0\n",
    "        try:\n",
    "            dset.load()\n",
    "            segment_dataset(dset)\n",
    "            if dset.edv >= 600 or dset.esv >= 600:\n",
    "                raise Exception(\"Prediction too large\")\n",
    "            p_edv = dset.edv\n",
    "            p_esv = dset.esv\n",
    "        except Exception as e:\n",
    "            log(\"***ERROR***: Exception %s thrown by dataset %s\" % (str(e), dset.name), 0)\n",
    "        submit_csv.write(\"%d_systolic,\" % int(dset.name))\n",
    "        for i in range(0, 600):\n",
    "            if i < p_esv:\n",
    "                submit_csv.write(\"0.0\")\n",
    "            else:\n",
    "                submit_csv.write(\"1.0\")\n",
    "            if i == 599:\n",
    "                submit_csv.write(\"\\n\")\n",
    "            else:\n",
    "                submit_csv.write(\",\")\n",
    "        submit_csv.write(\"%d_diastolic,\" % int(dset.name))\n",
    "        for i in range(0, 600):\n",
    "            if i < p_edv:\n",
    "                submit_csv.write(\"0.0\")\n",
    "            else:\n",
    "                submit_csv.write(\"1.0\")\n",
    "            if i == 599:\n",
    "                submit_csv.write(\"\\n\")\n",
    "            else:\n",
    "                submit_csv.write(\",\")\n",
    "        (edv, esv) = label_map.get(int(dset.name), (None, None))\n",
    "        if edv is not None:\n",
    "            accuracy_csv.write(\"%s,%f,%f,%f,%f\\n\" % (dset.name, edv, esv, p_edv, p_esv))\n",
    "\n",
    "    accuracy_csv.close()\n",
    "    submit_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "\n",
    "We provide the input directory which has the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-524011fd0028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mauto_segment_all_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/home/ubuntu/Notebooks/DataScienceBowl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-c345ff43c65b>\u001b[0m in \u001b[0;36mauto_segment_all_datasets\u001b[1;34m(dir)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mauto_segment_all_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     studies = next(os.walk(os.path.join(dir, \"train\")))[1] + next(\n\u001b[0m\u001b[0;32m     16\u001b[0m         os.walk(os.path.join(dir, \"validate\")))[1]\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dir = \"./data\"\n",
    "\n",
    "auto_segment_all_datasets(input_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
