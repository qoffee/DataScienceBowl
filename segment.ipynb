{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 3...\n",
      " Calculating rois...\n",
      "  Calculating mean...\n",
      "  Performing Fourier transforms...\n",
      "   Fourier transforming on slice 0...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 0...\n",
      "   Performing Gaussian blur on slice 0...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 1...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 1...\n",
      "   Performing Gaussian blur on slice 1...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 2...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 2...\n",
      "   Performing Gaussian blur on slice 2...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 3...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 3...\n",
      "   Performing Gaussian blur on slice 3...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 4...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 4...\n",
      "   Performing Gaussian blur on slice 4...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 5...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 5...\n",
      "   Performing Gaussian blur on slice 5...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 6...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 6...\n",
      "   Performing Gaussian blur on slice 6...\n",
      "result(256, 192)\n",
      "   Fourier transforming on slice 7...\n",
      "ff(30, 256, 192)\n",
      "first_harmonic(256, 192)\n",
      "   Inverse Fourier transforming on slice 7...\n",
      "   Performing Gaussian blur on slice 7...\n",
      "result(256, 192)\n",
      "h1s(8, 256, 192)\n",
      "  Applying regression filter...\n",
      "   Beginning iteration 0 of regression...\n",
      "c1[   2.59506062  141.20633339   78.20832239]\n",
      "   Beginning iteration 1 of regression...\n",
      "c1[   2.74060456  137.50428013   68.36912835]\n",
      "   Beginning iteration 2 of regression...\n",
      "c1[   2.80623335  133.49308918   67.01003735]\n",
      "   Beginning iteration 3 of regression...\n",
      "c1[   2.82844658  130.92388546   65.8951424 ]\n",
      "  Post-processing filtered images...\n",
      "   Performing final centroid regression...\n",
      "   Final image filtering...\n",
      "    Filtering image 1 of 8...\n",
      "    Filtering image 2 of 8...\n",
      "    Filtering image 3 of 8...\n",
      "    Filtering image 4 of 8...\n",
      "    Filtering image 5 of 8...\n",
      "    Filtering image 6 of 8...\n",
      "    Filtering image 7 of 8...\n",
      "    Filtering image 8 of 8...\n",
      "  Determining ROIs...\n",
      "   Getting ROI in slice 0...\n",
      "   Getting ROI in slice 1...\n",
      "   Getting ROI in slice 2...\n",
      "   Getting ROI in slice 3...\n",
      "   Getting ROI in slice 4...\n",
      "   Getting ROI in slice 5...\n",
      "   Getting ROI in slice 6...\n",
      "   Getting ROI in slice 7...\n",
      " Calculating areas...\n",
      "  Calculating areas at time 0...\n",
      "  Calculating areas at time 1...\n",
      "  Calculating areas at time 2...\n",
      "  Calculating areas at time 3...\n",
      "  Calculating areas at time 4...\n",
      "  Calculating areas at time 5...\n",
      "  Calculating areas at time 6...\n",
      "  Calculating areas at time 7...\n",
      "  Calculating areas at time 8...\n",
      "  Calculating areas at time 9...\n",
      "  Calculating areas at time 10...\n",
      "  Calculating areas at time 11...\n",
      "  Calculating areas at time 12...\n",
      "  Calculating areas at time 13...\n",
      "  Calculating areas at time 14...\n",
      "  Calculating areas at time 15...\n",
      "  Calculating areas at time 16...\n",
      "  Calculating areas at time 17...\n",
      "  Calculating areas at time 18...\n",
      "  Calculating areas at time 19...\n",
      "  Calculating areas at time 20...\n",
      "  Calculating areas at time 21...\n",
      "  Calculating areas at time 22...\n",
      "  Calculating areas at time 23...\n",
      "  Calculating areas at time 24...\n",
      "  Calculating areas at time 25...\n",
      "  Calculating areas at time 26...\n",
      "  Calculating areas at time 27...\n",
      "  Calculating areas at time 28...\n",
      "  Calculating areas at time 29...\n",
      " Calculating volumes...\n",
      " Calculating ef...\n",
      " Done, ef is 0.718424\n",
      "30 30\n",
      "60 60\n",
      "90 90\n",
      "120 120\n",
      "150 150\n",
      "180 180\n",
      "210 210\n",
      "240 240\n",
      "Processing 240 images and labels...\n",
      "BP 3\n",
      "<type 'numpy.ndarray'>\n",
      "***ERROR***: Exception 0 has type <type 'numpy.uint16'>, but expected one of: (<type 'float'>, <type 'int'>, <type 'long'>) thrown by dataset 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dicom\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from matplotlib import image\n",
    "from scipy.ndimage import label\n",
    "from scipy.ndimage.morphology import binary_erosion\n",
    "from scipy.fftpack import fftn, ifftn\n",
    "from scipy.signal import argrelmin, correlate\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import dicom, lmdb\n",
    "import fnmatch, subprocess\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
    "\n",
    "CAFFE_ROOT = \"/path/to/caffe_FCN/\"\n",
    "caffe_path = os.path.join(CAFFE_ROOT, \"python\")\n",
    "if caffe_path not in sys.path:\n",
    "    sys.path.insert(0, caffe_path)\n",
    "\n",
    "import caffe\n",
    "\n",
    "\n",
    "#\n",
    "# PARAMETERS\n",
    "#\n",
    "\n",
    "\n",
    "# number of bins to use in histogram for gaussian regression\n",
    "NUM_BINS = 100\n",
    "# number of standard deviations past which we will consider a pixel an outlier\n",
    "STD_MULTIPLIER = 2\n",
    "# number of points of our interpolated dataset to consider when searching for\n",
    "# a threshold value; the function by default is interpolated over 1000 points,\n",
    "# so 250 will look at the half of the points that is centered around the known\n",
    "# myocardium pixel\n",
    "THRESHOLD_AREA = 250\n",
    "# number of pixels on the line within which to search for a connected component\n",
    "# in a thresholded image, increase this to look for components further away\n",
    "COMPONENT_INDEX_TOLERANCE = 20\n",
    "# number of angles to search when looking for the correct orientation\n",
    "ANGLE_SLICES = 36\n",
    "\n",
    "\n",
    "#\n",
    "# FUNCTIONS\n",
    "#\n",
    "\n",
    "\n",
    "def log(msg, lvl):\n",
    "    string = \"\"\n",
    "    for i in range(lvl):\n",
    "        string += \" \"\n",
    "    string += msg\n",
    "    print string\n",
    "\n",
    "\n",
    "def auto_segment_all_datasets():\n",
    "    d = sys.argv[1]\n",
    "    studies = next(os.walk(os.path.join(d, \"train\")))[1] + next(\n",
    "        os.walk(os.path.join(d, \"validate\")))[1]\n",
    "\n",
    "    labels = np.loadtxt(os.path.join(d, \"train.csv\"), delimiter=\",\",\n",
    "                        skiprows=1)\n",
    "\n",
    "    label_map = {}\n",
    "    for l in labels:\n",
    "        label_map[l[0]] = (l[2], l[1])\n",
    "\n",
    "    num_samples = None\n",
    "    if len(sys.argv) > 2:\n",
    "        num_samples = int(sys.argv[2])\n",
    "        studies = random.sample(studies, num_samples)\n",
    "    if os.path.exists(\"output\"):\n",
    "        shutil.rmtree(\"output\")\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "    accuracy_csv = open(\"accuracy.csv\", \"w\")\n",
    "    accuracy_csv.write(\"Dataset,Actual EDV,Actual ESV,Predicted EDV,\"\n",
    "                       \"Predicted ESV\\n\")\n",
    "    submit_csv = open(\"submit.csv\", \"w\")\n",
    "    submit_csv.write(\"Id,\")\n",
    "    for i in range(0, 600):\n",
    "        submit_csv.write(\"P%d\" % i)\n",
    "        if i != 599:\n",
    "            submit_csv.write(\",\")\n",
    "        else:\n",
    "            submit_csv.write(\"\\n\")\n",
    "\n",
    "    for s in studies:\n",
    "        if int(s) <= 500:\n",
    "            full_path = os.path.join(d, \"train\", s)\n",
    "        else:\n",
    "            full_path = os.path.join(d, \"validate\", s)\n",
    "\n",
    "        dset = Dataset(full_path, s)\n",
    "        print \"Processing dataset %s...\" % dset.name\n",
    "        p_edv = 0\n",
    "        p_esv = 0\n",
    "        try:\n",
    "            dset.load()\n",
    "            segment_dataset(dset)\n",
    "            if dset.edv >= 600 or dset.esv >= 600:\n",
    "                raise Exception(\"Prediction too large\")\n",
    "            p_edv = dset.edv\n",
    "            p_esv = dset.esv\n",
    "        except Exception as e:\n",
    "            log(\"***ERROR***: Exception %s thrown by dataset %s\" % (str(e), dset.name), 0)\n",
    "        submit_csv.write(\"%d_systolic,\" % int(dset.name))\n",
    "        for i in range(0, 600):\n",
    "            if i < p_esv:\n",
    "                submit_csv.write(\"0.0\")\n",
    "            else:\n",
    "                submit_csv.write(\"1.0\")\n",
    "            if i == 599:\n",
    "                submit_csv.write(\"\\n\")\n",
    "            else:\n",
    "                submit_csv.write(\",\")\n",
    "        submit_csv.write(\"%d_diastolic,\" % int(dset.name))\n",
    "        for i in range(0, 600):\n",
    "            if i < p_edv:\n",
    "                submit_csv.write(\"0.0\")\n",
    "            else:\n",
    "                submit_csv.write(\"1.0\")\n",
    "            if i == 599:\n",
    "                submit_csv.write(\"\\n\")\n",
    "            else:\n",
    "                submit_csv.write(\",\")\n",
    "        (edv, esv) = label_map.get(int(dset.name), (None, None))\n",
    "        if edv is not None:\n",
    "            accuracy_csv.write(\"%s,%f,%f,%f,%f\\n\" % (dset.name, edv, esv, p_edv, p_esv))\n",
    "\n",
    "    accuracy_csv.close()\n",
    "    submit_csv.close()\n",
    "\n",
    "class Dataset(object):\n",
    "    dataset_count = 0\n",
    "\n",
    "    def __init__(self, directory, subdir):\n",
    "        # deal with any intervening directories\n",
    "        while True:\n",
    "            subdirs = next(os.walk(directory))[1]\n",
    "            if len(subdirs) == 1:\n",
    "                directory = os.path.join(directory, subdirs[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        slices = []\n",
    "        for s in subdirs:\n",
    "            m = re.match(\"sax_(\\d+)\", s)\n",
    "            if m is not None:\n",
    "                slices.append(int(m.group(1)))\n",
    "\n",
    "        slices_map = {}\n",
    "        first = True\n",
    "        times = []\n",
    "        for s in slices:\n",
    "            files = next(os.walk(os.path.join(directory, \"sax_%d\" % s)))[2]\n",
    "            offset = None\n",
    "\n",
    "            for f in files:\n",
    "                m = re.match(\"IM-(\\d{4,})-(\\d{4})\\.dcm\", f)\n",
    "                if m is not None:\n",
    "                    if first:\n",
    "                        times.append(int(m.group(2)))\n",
    "                    if offset is None:\n",
    "                        offset = int(m.group(1))\n",
    "\n",
    "            first = False\n",
    "            slices_map[s] = offset\n",
    "\n",
    "        self.directory = directory\n",
    "        self.time = sorted(times)\n",
    "        self.slices = sorted(slices)\n",
    "        self.slices_map = slices_map\n",
    "        Dataset.dataset_count += 1\n",
    "        self.name = subdir\n",
    "\n",
    "    def _filename(self, s, t):\n",
    "        return os.path.join(self.directory,\"sax_%d\" % s, \"IM-%04d-%04d.dcm\" % (self.slices_map[s], t))\n",
    "\n",
    "    def _read_dicom_image(self, filename):\n",
    "        d = dicom.read_file(filename)\n",
    "        img = d.pixel_array\n",
    "        return np.array(img)\n",
    "\n",
    "    def _read_all_dicom_images(self):\n",
    "        f1 = self._filename(self.slices[0], self.time[0])\n",
    "        d1 = dicom.read_file(f1)\n",
    "        (x, y) = d1.PixelSpacing\n",
    "        (x, y) = (float(x), float(y))\n",
    "        f2 = self._filename(self.slices[1], self.time[0])\n",
    "        d2 = dicom.read_file(f2)\n",
    "\n",
    "        # try a couple of things to measure distance between slices\n",
    "        try:\n",
    "            dist = np.abs(d2.SliceLocation - d1.SliceLocation)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                dist = d1.SliceThickness\n",
    "            except AttributeError:\n",
    "                dist = 8  # better than nothing...\n",
    "\n",
    "        self.images = np.array([[self._read_dicom_image(self._filename(d, i))\n",
    "                                 for i in self.time]\n",
    "                                for d in self.slices])\n",
    "        self.dist = dist\n",
    "        self.area_multiplier = x * y\n",
    "\n",
    "    def load(self):\n",
    "        self._read_all_dicom_images()\n",
    "\n",
    "\n",
    "# assumes dataset is loaded, call dataset.load()\n",
    "def segment_dataset(dataset):\n",
    "    images = dataset.images\n",
    "    dist = dataset.dist\n",
    "    areaMultiplier = dataset.area_multiplier\n",
    "    # shape: num slices, num snapshots, rows, columns\n",
    "    log(\"Calculating rois...\", 1)\n",
    "    rois, circles = calc_rois(images)\n",
    "    log(\"Calculating areas...\", 1)\n",
    "    all_masks, all_areas = calc_all_areas(images, rois, circles)\n",
    "    log(\"Calculating volumes...\", 1)\n",
    "    area_totals = [calc_total_volume(a, areaMultiplier, dist)\n",
    "                   for a in all_areas]\n",
    "    log(\"Calculating ef...\", 1)\n",
    "    edv = max(area_totals)\n",
    "    esv = min(area_totals)\n",
    "    ef = (edv - esv) / edv\n",
    "    log(\"Done, ef is %f\" % ef, 1)\n",
    "\n",
    "    save_masks_to_dir(dataset, all_masks)\n",
    "\n",
    "    output = {}\n",
    "    output[\"edv\"] = edv\n",
    "    output[\"esv\"] = esv\n",
    "    output[\"ef\"] = ef\n",
    "    output[\"areas\"] = all_areas.tolist()\n",
    "    f = open(\"output/%s/output.json\" % dataset.name, \"w\")\n",
    "    json.dump(output, f, indent=2)\n",
    "    f.close()\n",
    "    dataset.edv = edv\n",
    "    dataset.esv = esv\n",
    "    dataset.ef = ef\n",
    "\n",
    "\"\"\"\n",
    "def save_masks_to_dir(dataset, all_masks):\n",
    "    os.mkdir(\"output/%s\" % dataset.name)\n",
    "    for t in range(len(dataset.time)):\n",
    "        os.mkdir(\"output/%s/time%02d\" % (dataset.name, t))\n",
    "        for s in range(len(dataset.slices)):\n",
    "            mask = all_masks[t][s]\n",
    "            image.imsave(\"output/%s/time%02d/slice%02d_mask.png\" %\n",
    "                         (dataset.name, t, s), mask)\n",
    "            eroded = binary_erosion(mask)\n",
    "            hollow_mask = np.where(eroded, 0, mask)\n",
    "            colorimg = cv2.cvtColor(dataset.images[s][t],\n",
    "                                    cv2.COLOR_GRAY2RGB)\n",
    "            colorimg = colorimg.astype(np.uint8)\n",
    "            colorimg[hollow_mask != 0] = [255, 0, 255]\n",
    "            image.imsave(\"output/%s/time%02d/slice%02d_color.png\" %\n",
    "                         (dataset.name, t, s), colorimg)\n",
    "\"\"\"\n",
    "\n",
    "def save_masks_to_dir(dataset, all_masks):\n",
    "    imgs, labels = [], []\n",
    "    os.mkdir(\"output/%s\" % dataset.name)\n",
    "    if os.path.exists(\"input\"):\n",
    "        shutil.rmtree(\"input\")\n",
    "    os.mkdir(\"input\")\n",
    "    os.mkdir(\"input/%s\" % dataset.name)\n",
    "    for s in range(len(dataset.slices)):\n",
    "        os.mkdir(\"output/%s/slice%02d\" % (dataset.name, s))\n",
    "        os.mkdir(\"input/%s/slice%02d\" % (dataset.name, s))\n",
    "        for t in range(len(dataset.time)):\n",
    "            mask = all_masks[t][s]\n",
    "            image.imsave(\"output/%s/slice%02d/time%02d_mask.png\" %\n",
    "                         (dataset.name, s, t), mask)\n",
    "            labels.append(mask)\n",
    "            image_save = dataset.images[s][t]\n",
    "            image.imsave(\"input/%s/slice%02d/time%02d_DICOM.png\" %\n",
    "                         (dataset.name, s, t), image_save)\n",
    "            imgs.append(image_save)\n",
    "            \"\"\"\"\n",
    "            eroded = binary_erosion(mask)\n",
    "            hollow_mask = np.where(eroded, 0, mask)\n",
    "            colorimg = cv2.cvtColor(dataset.images[s][t],\n",
    "                                    cv2.COLOR_GRAY2RGB)\n",
    "            colorimg = colorimg.astype(np.uint8)\n",
    "            colorimg[hollow_mask != 0] = [255, 0, 255]\n",
    "            image.imsave(\"output/%s/time%02d/slice%02d_color.png\" %\n",
    "                         (dataset.name, t, s), colorimg)\n",
    "            \"\"\"\n",
    "        print len(imgs), len(labels)\n",
    "        \n",
    "###############################################################   \n",
    "\n",
    "    for lmdb_name in [\"lmdb_img_name\", \"lmdb_label_name\"]:\n",
    "        db_path = os.path.abspath(lmdb_name)\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "    counter_img = 0\n",
    "    counter_label = 0\n",
    "    batchsz = 100\n",
    "    print(\"Processing {:d} images and labels...\".format(len(imgs)))\n",
    "    for i in xrange(int(np.ceil(len(imgs) / float(batchsz)))):\n",
    "#         print \"we are here\"\n",
    "        batch_imgs = imgs[(batchsz*i):(batchsz*(i+1))]\n",
    "#         print \"BP 1\"\n",
    "        batch_labels = labels[(batchsz*i):(batchsz*(i+1))]\n",
    "#         print \"BP 2\"\n",
    "\n",
    "        if len(batch_imgs) == 0:\n",
    "            break\n",
    "#         imgs, labels = [], []\n",
    "#         for idx,ctr in enumerate(batch):\n",
    "#             try:\n",
    "#                 img, label = load_contour(ctr, img_path)\n",
    "#                 imgs.append(img)\n",
    "#                 labels.append(label)\n",
    "#                 if idx % 20 == 0:\n",
    "#                     print ctr\n",
    "#                     plt.imshow(img)\n",
    "#                     plt.show()\n",
    "#                     plt.imshow(label)\n",
    "#                     plt.show()\n",
    "#             except IOError:\n",
    "#                 continue\n",
    "        db_imgs = lmdb.open(\"lmdb_img_name\", map_size=1e12)\n",
    "        with db_imgs.begin(write=True) as txn_img:          \n",
    "            for img in batch_imgs:\n",
    "                print \"BP 3\"\n",
    "                print(type(img))\n",
    "                datum = caffe.io.array_to_datum(np.expand_dims(img, axis=0))\n",
    "                print \"BP 4\"\n",
    "                txn_img.put(\"{:0>10d}\".format(counter_img), datum.SerializeToString())\n",
    "                print \"BP 5\"\n",
    "                counter_img += 1\n",
    "        print(\"Processed {:d} images\".format(counter_img))\n",
    "        db_labels = lmdb.open(\"lmdb_label_name\", map_size=1e12)\n",
    "        with db_labels.begin(write=True) as txn_label:\n",
    "            for lbl in batch_labels:\n",
    "                print \"BP 6\"\n",
    "                datum = caffe.io.array_to_datum(np.expand_dims(lbl, axis=0))\n",
    "                txn_label.put(\"{:0>10d}\".format(counter_label), datum.SerializeToString())\n",
    "                counter_label += 1\n",
    "        print(\"Processed {:d} labels\".format(counter_label))\n",
    "\n",
    "def calc_rois(images):\n",
    "    (num_slices, _, _, _) = images.shape\n",
    "    log(\"Calculating mean...\", 2)\n",
    "    dc = np.mean(images, 1)\n",
    "\n",
    "    def get_H1(i):\n",
    "        log(\"Fourier transforming on slice %d...\" % i, 3)\n",
    "        ff = fftn(images[i])\n",
    "        print \"ff\"+str(np.shape(ff))\n",
    "        first_harmonic = ff[1, :, :]\n",
    "        print \"first_harmonic\"+str(np.shape(first_harmonic))\n",
    "        log(\"Inverse Fourier transforming on slice %d...\" % i, 3)\n",
    "        result = np.absolute(ifftn(first_harmonic))\n",
    "        log(\"Performing Gaussian blur on slice %d...\" % i, 3)\n",
    "        result = cv2.GaussianBlur(result, (5, 5), 0)\n",
    "        print \"result\" + str(np.shape(result))\n",
    "        return result\n",
    "\n",
    "    log(\"Performing Fourier transforms...\", 2)\n",
    "    h1s = np.array([get_H1(i) for i in range(num_slices)])\n",
    "    print \"h1s\" + str(np.shape(h1s))\n",
    "    m = np.max(h1s) * 0.05\n",
    "    h1s[h1s < m] = 0\n",
    "\n",
    "    log(\"Applying regression filter...\", 2)\n",
    "    regress_h1s = regression_filter(h1s)\n",
    "    log(\"Post-processing filtered images...\", 2)\n",
    "    proc_regress_h1s, coords = post_process_regression(regress_h1s)\n",
    "    log(\"Determining ROIs...\", 2)\n",
    "    rois, circles = get_ROIs(dc, proc_regress_h1s, coords)\n",
    "    return rois, circles\n",
    "\n",
    "\n",
    "def calc_all_areas(images, rois, circles):\n",
    "    closest_slice = get_closest_slice(rois)\n",
    "    (_, times, _, _) = images.shape\n",
    "\n",
    "    def calc_areas(time):\n",
    "        log(\"Calculating areas at time %d...\" % time, 2)\n",
    "        mask, mean = locate_lv_blood_pool(images, rois, circles, closest_slice,\n",
    "                                          time)\n",
    "        masks, areas = propagate_segments(images, rois, mask, mean,\n",
    "                                          closest_slice, time)\n",
    "        return (masks, areas)\n",
    "\n",
    "    result = np.transpose(map(calc_areas, range(times)))\n",
    "    all_masks = result[0]\n",
    "    all_areas = result[1]\n",
    "    return all_masks, all_areas\n",
    "\n",
    "\n",
    "def calc_total_volume(areas, area_multiplier, dist):\n",
    "    slices = np.array(sorted(areas.keys()))\n",
    "    modified = [areas[i] * area_multiplier for i in slices]\n",
    "    vol = 0\n",
    "    for i in slices[:-1]:\n",
    "        a, b = modified[i], modified[i+1]\n",
    "        subvol = (dist/3.0) * (a + np.sqrt(a*b) + b)\n",
    "        vol += subvol / 1000.0  # conversion to mL\n",
    "    return vol\n",
    "\n",
    "\n",
    "def get_centroid(img):\n",
    "    nz = np.nonzero(img)\n",
    "    pxls = np.transpose(nz)\n",
    "    weights = img[nz]\n",
    "    avg = np.average(pxls, axis=0, weights=weights)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def regress_centroids(cs):\n",
    "    num_slices = len(cs)\n",
    "    y_centroids = cs[:, 0]\n",
    "    x_centroids = cs[:, 1]\n",
    "    z_values = np.array(range(num_slices))\n",
    "\n",
    "    (xslope, xintercept, _, _, _) = linregress(z_values, x_centroids)\n",
    "    (yslope, yintercept, _, _, _) = linregress(z_values, y_centroids)\n",
    "\n",
    "    return (xslope, xintercept, yslope, yintercept)\n",
    "\n",
    "\n",
    "def get_weighted_distances(imgs, coords, xs, xi, ys, yi):\n",
    "    a = np.array([0, yi, xi])\n",
    "    n = np.array([1, ys, xs])\n",
    "\n",
    "    zeros = np.zeros(3)\n",
    "\n",
    "    def dist(p):\n",
    "        to_line = (a - p) - (np.dot((a - p), n) * n)\n",
    "        d = euclidean(zeros, to_line)\n",
    "        return d\n",
    "\n",
    "    def weight(p):\n",
    "        (z, y, x) = p\n",
    "        return imgs[z, y, x]\n",
    "\n",
    "    dists = np.array([dist(c) for c in coords])\n",
    "    weights = np.array([weight(c) for c in coords])\n",
    "    return (coords, dists, weights)\n",
    "\n",
    "\n",
    "def gaussian_fit(dists, weights):\n",
    "    # based on http://stackoverflow.com/questions/11507028/fit-a-gaussian-function\n",
    "    (x, y) = histogram_transform(dists, weights)\n",
    "    fivep = int(len(x) * 0.05)\n",
    "    xtmp = x\n",
    "    ytmp = y\n",
    "    fromFront = False\n",
    "    while True:\n",
    "        if len(xtmp) == 0 and len(ytmp) == 0:\n",
    "            if fromFront:\n",
    "                # well we failed\n",
    "                idx = np.argmax(y)\n",
    "                xmax = x[idx]\n",
    "                p0 = [max(y), xmax, xmax]\n",
    "                (A, mu, sigma) = p0\n",
    "                return mu, sigma, lambda x: gauss(x, A, mu, sigma)\n",
    "            else:\n",
    "                fromFront = True\n",
    "                xtmp = x\n",
    "                ytmp = y\n",
    "\n",
    "        idx = np.argmax(ytmp)\n",
    "        xmax = xtmp[idx]\n",
    "\n",
    "        def gauss(x, *p):\n",
    "            A, mu, sigma = p\n",
    "            return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "        p0 = [max(ytmp), xmax, xmax]\n",
    "        try:\n",
    "            coeff, var_matrix = curve_fit(gauss, xtmp, ytmp, p0=p0)\n",
    "            (A, mu, sigma) = coeff\n",
    "            return (mu, sigma, lambda x: gauss(x, A, mu, sigma))\n",
    "        except RuntimeError:\n",
    "            if fromFront:\n",
    "                xtmp = xtmp[fivep:]\n",
    "                ytmp = ytmp[fivep:]\n",
    "            else:\n",
    "                xtmp = xtmp[:-fivep]\n",
    "                ytmp = ytmp[:-fivep]\n",
    "\n",
    "\n",
    "def histogram_transform(values, weights):\n",
    "    hist, bins = np.histogram(values, bins=NUM_BINS, weights=weights)\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    bin_centers = bins[:-1] + (bin_width / 2)\n",
    "\n",
    "    return (bin_centers, hist)\n",
    "\n",
    "\n",
    "def get_outliers(coords, dists, weights):\n",
    "    fivep = int(len(weights) * 0.05)\n",
    "    ctr = 1\n",
    "    while True:\n",
    "        (mean, std, fn) = gaussian_fit(dists, weights)\n",
    "        low_values = dists < (mean - STD_MULTIPLIER*np.abs(std))\n",
    "        high_values = dists > (mean + STD_MULTIPLIER*np.abs(std))\n",
    "        outliers = np.logical_or(low_values, high_values)\n",
    "        if len(coords[outliers]) == len(coords):\n",
    "            weights[-fivep*ctr:] = 0\n",
    "            ctr += 1\n",
    "        else:\n",
    "            return coords[outliers]\n",
    "\n",
    "\n",
    "def regress_and_filter_distant(imgs):\n",
    "    centroids = np.array([get_centroid(img) for img in imgs])\n",
    "    raw_coords = np.transpose(np.nonzero(imgs))\n",
    "    (xslope, xintercept, yslope, yintercept) = regress_centroids(centroids)\n",
    "    (coords, dists, weights) = get_weighted_distances(imgs, raw_coords, xslope,\n",
    "                                                      xintercept, yslope,\n",
    "                                                      yintercept)\n",
    "    outliers = get_outliers(coords, dists, weights)\n",
    "    imgs_cpy = np.copy(imgs)\n",
    "    for c in outliers:\n",
    "        (z, x, y) = c\n",
    "        imgs_cpy[z, x, y] = 0\n",
    "    return imgs_cpy\n",
    "\n",
    "\n",
    "def regression_filter(imgs):\n",
    "    condition = True\n",
    "    iternum = 0\n",
    "    while(condition):\n",
    "        log(\"Beginning iteration %d of regression...\" % iternum, 3)\n",
    "        iternum += 1\n",
    "        imgs_filtered = regress_and_filter_distant(imgs)\n",
    "        c1 = get_centroid(imgs)\n",
    "        print \"c1\" + str(c1)\n",
    "        c2 = get_centroid(imgs_filtered)\n",
    "        dc = np.linalg.norm(c1 - c2)\n",
    "        imgs = imgs_filtered\n",
    "        condition = (dc > 1.0)  # because python has no do-while loops\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def post_process_regression(imgs):\n",
    "    (numimgs, _, _) = imgs.shape\n",
    "    centroids = np.array([get_centroid(img) for img in imgs])\n",
    "    log(\"Performing final centroid regression...\", 3)\n",
    "    (xslope, xintercept, yslope, yintercept) = regress_centroids(centroids)\n",
    "    imgs_cpy = np.copy(imgs)\n",
    "\n",
    "    def filter_one_img(zlvl):\n",
    "        points_on_zlvl = np.transpose(imgs[zlvl].nonzero())\n",
    "        points_on_zlvl = np.insert(points_on_zlvl, 0, zlvl, axis=1)\n",
    "        (coords, dists, weights) = get_weighted_distances(imgs, points_on_zlvl,\n",
    "                                                          xslope, xintercept,\n",
    "                                                          yslope, yintercept)\n",
    "        outliers = get_outliers(coords, dists, weights)\n",
    "        for c in outliers:\n",
    "            (z, x, y) = c\n",
    "            imgs_cpy[z, x, y] = 0\n",
    "\n",
    "    log(\"Final image filtering...\", 3)\n",
    "    for z in range(numimgs):\n",
    "        log(\"Filtering image %d of %d...\" % (z+1, numimgs), 4)\n",
    "        filter_one_img(z)\n",
    "\n",
    "    return (imgs_cpy, (xslope, xintercept, yslope, yintercept))\n",
    "\n",
    "\n",
    "def floats_draw_circle(img, center, r, color, thickness):\n",
    "    (x, y) = center\n",
    "    x, y = int(np.round(x)), int(np.round(y))\n",
    "    r = int(np.round(r))\n",
    "    cv2.circle(img, center=(x, y), radius=r, color=color, thickness=thickness)\n",
    "\n",
    "\n",
    "def filled_ratio_of_circle(img, center, r):\n",
    "    mask = np.zeros_like(img)\n",
    "    floats_draw_circle(mask, center, r, 1, -1)\n",
    "    masked = mask * img\n",
    "    (x, _) = np.nonzero(mask)\n",
    "    (x2, _) = np.nonzero(masked)\n",
    "    if x.size == 0:\n",
    "        return 0\n",
    "    return float(x2.size) / x.size\n",
    "\n",
    "\n",
    "def circle_smart_radius(img, center):\n",
    "    domain = np.arange(1, 100)\n",
    "    (xintercept, yintercept) = center\n",
    "\n",
    "    def ratio(r):\n",
    "        return filled_ratio_of_circle(img, (xintercept, yintercept), r)*r\n",
    "\n",
    "    y = np.array([ratio(d) for d in domain])\n",
    "    most = np.argmax(y)\n",
    "    return domain[most]\n",
    "\n",
    "\n",
    "def get_ROIs(originals, h1s, regression_params):\n",
    "    (xslope, xintercept, yslope, yintercept) = regression_params\n",
    "    (num_slices, _, _) = h1s.shape\n",
    "    results = []\n",
    "    circles = []\n",
    "    for i in range(num_slices):\n",
    "        log(\"Getting ROI in slice %d...\" % i, 3)\n",
    "        o = originals[i]\n",
    "        h = h1s[i]\n",
    "        ctr = (xintercept + xslope * i, yintercept + yslope * i)\n",
    "        r = circle_smart_radius(h, ctr)\n",
    "        tmp = np.zeros_like(o)\n",
    "        floats_draw_circle(tmp, ctr, r, 1, -1)\n",
    "        results.append(tmp * o)\n",
    "        circles.append((ctr, r))\n",
    "\n",
    "    return (np.array(results), np.array(circles))\n",
    "\n",
    "\n",
    "def bresenham(x0, x1, y0, y1, fn):\n",
    "    # using some pseudocode from\n",
    "    # https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm\n",
    "    # and also https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm\n",
    "    steep = abs(y1-y0) > abs(x1-x0)\n",
    "    if steep:\n",
    "        x0, x1, y0, y1 = y0, y1, x0, x1\n",
    "    if x0 > x1:\n",
    "        x0, x1, y0, y1 = x1, x0, y1, y0\n",
    "\n",
    "    def plot(x, y):\n",
    "        if steep:\n",
    "            fn(y, x)\n",
    "        else:\n",
    "            fn(x, y)\n",
    "\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "\n",
    "    D = 2*np.abs(dy) - dx\n",
    "    plot(x0, y0)\n",
    "    y = y0\n",
    "\n",
    "    for x in range(x0+1, x1+1):  # x0+1 to x1\n",
    "        D = D + 2*np.abs(dy)\n",
    "        if D > 0:\n",
    "            y += np.sign(dy)\n",
    "            D -= 2*dx\n",
    "        plot(x, y)\n",
    "\n",
    "\n",
    "def line_thru(bounds, center, theta):\n",
    "    (xmin, xmax, ymin, ymax) = bounds\n",
    "    (cx, cy) = center\n",
    "\n",
    "    if np.cos(theta) == 0:\n",
    "        return (cx, ymin, cx, ymax)\n",
    "    slope = np.tan(theta)\n",
    "\n",
    "    x0 = xmin\n",
    "    y0 = cy - (cx - xmin) * slope\n",
    "    if y0 < ymin:\n",
    "        y0 = ymin\n",
    "        x0 = max(xmin, cx - ((cy - ymin) / slope))\n",
    "    elif y0 > ymax:\n",
    "        y0 = ymax\n",
    "        x0 = max(xmin, cx - ((cy - ymax) / slope))\n",
    "\n",
    "    x1 = xmax\n",
    "    y1 = cy + (xmax - cx) * slope\n",
    "    if y1 < ymin:\n",
    "        y1 = ymin\n",
    "        x1 = min(xmax, cx + ((ymin - cy) / slope))\n",
    "    elif y1 > ymax:\n",
    "        y1 = ymax\n",
    "        x1 = min(xmax, cx + ((ymax - cy) / slope))\n",
    "\n",
    "    return (x0, x1, y0, y1)\n",
    "\n",
    "\n",
    "def get_line_coords(w, h, cx, cy, theta):\n",
    "    coords = np.floor(np.array(line_thru((0, w-1, 0, h-1), (cx, cy), theta)))\n",
    "    return coords.astype(np.int)\n",
    "\n",
    "\n",
    "def trim_zeros_indices(has_zeros):\n",
    "    first = 0\n",
    "    for i in has_zeros:\n",
    "        if i == 0:\n",
    "            first += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    last = len(has_zeros)\n",
    "    for i in has_zeros[::-1]:\n",
    "        if i == 0:\n",
    "            last -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return first, last\n",
    "\n",
    "\n",
    "def get_line(roi, cx, cy, theta):\n",
    "    (h, w) = roi.shape\n",
    "    (x0, x1, y0, y1) = get_line_coords(w, h, cx, cy, theta)\n",
    "\n",
    "    intensities = []\n",
    "    coords = []\n",
    "\n",
    "    def collect(x, y):\n",
    "        if y < 0 or y >= h or x < 0 or x >= w:\n",
    "            return\n",
    "        intensities.append(roi[y, x])\n",
    "        coords.append((x, y))\n",
    "\n",
    "    bresenham(x0, x1, y0, y1, collect)\n",
    "\n",
    "    def geti(idx):\n",
    "        return intensities[idx]\n",
    "\n",
    "    getiv = np.vectorize(geti)\n",
    "    x = np.arange(0, len(intensities))\n",
    "    y = getiv(x)\n",
    "    first, last = trim_zeros_indices(y)\n",
    "    trimy = y[first:last]\n",
    "    trimcoords = coords[first:last]\n",
    "\n",
    "    trimx = np.arange(0, trimy.size)\n",
    "    return (trimx, trimy, trimcoords)\n",
    "\n",
    "\n",
    "def find_best_angle(roi, circ):\n",
    "    ((cx, cy), r) = circ\n",
    "    results = np.zeros(ANGLE_SLICES)\n",
    "    fns = [None for i in range(ANGLE_SLICES)]\n",
    "\n",
    "    COS_MATCHED_FILTER_FREQ = 2.5\n",
    "\n",
    "    def score_matched(trimx, trimy):\n",
    "        # first, normalize this data\n",
    "        newtrimx = np.linspace(0.0, 1.0, np.size(trimx))\n",
    "        minimum = np.min(trimy)\n",
    "        maximum = np.max(trimy) - minimum\n",
    "        newtrimy = (trimy - minimum) / maximum\n",
    "\n",
    "        filt = 1 - ((np.cos(COS_MATCHED_FILTER_FREQ*2*np.pi*newtrimx)) /\n",
    "                    2 + (0.5))\n",
    "        cr = correlate(newtrimy, filt, mode=\"same\")\n",
    "        return np.max(cr)\n",
    "\n",
    "    for i in range(ANGLE_SLICES):\n",
    "        trimx, trimy, trimcoords = get_line(roi, cx, cy, np.pi*i/ANGLE_SLICES)\n",
    "        score2 = score_matched(trimx, trimy)\n",
    "        results[i] = score2\n",
    "        fns[i] = (UnivariateSpline(trimx, trimy), trimx, trimcoords)\n",
    "\n",
    "    best = np.argmax(results)\n",
    "    return (best * np.pi / ANGLE_SLICES, fns[best])\n",
    "\n",
    "\n",
    "def find_threshold_point(best, best_fn):\n",
    "    fn, trimx, trim_coords = best_fn\n",
    "    dom = np.linspace(np.min(trimx), np.max(trimx), 1000)\n",
    "    f = fn(dom)\n",
    "    mins = argrelmin(f)\n",
    "\n",
    "    closest_min = -1\n",
    "    closest_dist = -1\n",
    "    for m in np.nditer(mins):\n",
    "        dist = np.abs(500 - m)\n",
    "        if closest_min == -1 or closest_dist > dist:\n",
    "            closest_min = m\n",
    "            closest_dist = dist\n",
    "\n",
    "    fnprime = fn.derivative()\n",
    "    restrict = dom[np.max(closest_min-THRESHOLD_AREA, 0):\n",
    "                   closest_min+THRESHOLD_AREA]\n",
    "    f2 = fnprime(restrict)\n",
    "\n",
    "    m1 = restrict[np.argmax(f2)]\n",
    "    mean = fn(m1)\n",
    "\n",
    "    idx = np.min([int(np.floor(m1))+1, len(trim_coords)-1])\n",
    "    return (mean, trim_coords, idx)\n",
    "\n",
    "\n",
    "def get_closest_slice(rois):\n",
    "    ctrd = get_centroid(rois)\n",
    "    closest_slice = int(np.round(ctrd[0]))\n",
    "    return closest_slice\n",
    "\n",
    "\n",
    "def locate_lv_blood_pool(images, rois, circles, closest_slice, time):\n",
    "    best, best_fn = find_best_angle(rois[closest_slice],\n",
    "                                    circles[closest_slice])\n",
    "    mean, coords, idx = find_threshold_point(best, best_fn)\n",
    "    thresh, img_bin = cv2.threshold(images[closest_slice,\n",
    "                                           time].astype(np.float32),\n",
    "                                    mean, 255.0, cv2.THRESH_BINARY)\n",
    "    labeled, num = label(img_bin)\n",
    "    x, y = coords[idx]\n",
    "\n",
    "    count = 0\n",
    "    # Look along the line for a component. If one isn't found within a certain\n",
    "    # number of indices, just spit out the original coordinate.\n",
    "    while labeled[y][x] == 0:\n",
    "        idx += 1\n",
    "        count += 1\n",
    "        x, y = coords[idx]\n",
    "        if count > COMPONENT_INDEX_TOLERANCE:\n",
    "            idx -= count\n",
    "            x, y = coords[idx]\n",
    "            break\n",
    "\n",
    "    if count <= COMPONENT_INDEX_TOLERANCE:\n",
    "        component = np.transpose(np.nonzero(labeled == labeled[y][x]))\n",
    "    else:\n",
    "        component = np.array([[y, x]])\n",
    "\n",
    "    hull = cv2.convexHull(component)\n",
    "    squeezed = hull\n",
    "    if count <= COMPONENT_INDEX_TOLERANCE:\n",
    "        squeezed = np.squeeze(squeezed)\n",
    "    hull = np.fliplr(squeezed)\n",
    "\n",
    "    mask = np.zeros_like(labeled)\n",
    "    cv2.drawContours(mask, [hull], 0, 255, thickness=-1)\n",
    "    return mask, mean\n",
    "\n",
    "\n",
    "def propagate_segments(images, rois, base_mask, mean, closest_slice, time):\n",
    "    def propagate_segment(i, mask):\n",
    "        thresh, img_bin = cv2.threshold(images[i,\n",
    "                                               time].astype(np.float32),\n",
    "                                        mean, 255.0, cv2.THRESH_BINARY)\n",
    "\n",
    "        labeled, features = label(img_bin)\n",
    "\n",
    "        region1 = mask == 255\n",
    "        max_similar = -1\n",
    "        max_region = 0\n",
    "        for j in range(1, features+1):\n",
    "            region2 = labeled == j\n",
    "            intersect = np.count_nonzero(np.logical_and(region1, region2))\n",
    "            union = np.count_nonzero(np.logical_or(region1, region2))\n",
    "            similar = float(intersect) / union\n",
    "            if max_similar == -1 or max_similar < similar:\n",
    "                max_similar = similar\n",
    "                max_region = j\n",
    "\n",
    "        if max_similar == 0:\n",
    "            component = np.transpose(np.nonzero(mask))\n",
    "        else:\n",
    "            component = np.transpose(np.nonzero(labeled == max_region))\n",
    "        hull = cv2.convexHull(component)\n",
    "        hull = np.squeeze(hull)\n",
    "        if hull.shape == (2L,):\n",
    "            hull = np.array([hull])\n",
    "        hull = np.fliplr(hull)\n",
    "\n",
    "        newmask = np.zeros_like(img_bin)\n",
    "\n",
    "        cv2.drawContours(newmask, [hull], 0, 255, thickness=-1)\n",
    "\n",
    "        return newmask\n",
    "\n",
    "    (rois_depth, _, _) = rois.shape\n",
    "    newmask = base_mask\n",
    "    masks = {}\n",
    "    areas = {}\n",
    "    masks[closest_slice] = base_mask\n",
    "    areas[closest_slice] = np.count_nonzero(base_mask)\n",
    "    for i in range(closest_slice-1, -1, -1):\n",
    "        newmask = propagate_segment(i, newmask)\n",
    "        masks[i] = newmask\n",
    "        areas[i] = np.count_nonzero(newmask)\n",
    "\n",
    "    newmask = base_mask\n",
    "    for i in range(closest_slice+1, rois_depth):\n",
    "        newmask = propagate_segment(i, newmask)\n",
    "        masks[i] = newmask\n",
    "        areas[i] = np.count_nonzero(newmask)\n",
    "\n",
    "    return masks, areas\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.argv[1] = \"/home/tyagi/Desktop/SML\"\n",
    "    sys.argv[2] = 1\n",
    "    random.seed()\n",
    "    auto_segment_all_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
